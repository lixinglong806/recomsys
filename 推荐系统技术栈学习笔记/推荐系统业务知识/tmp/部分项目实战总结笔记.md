### 1. 分箱

```python
# pd.cut() 等距切分
import math
# 对于age字段，分成6段 [-math.inf, 25,40,50,60,70, math.inf]
ages_bins = [-math.inf, 25,40,50,60,70, math.inf] # 左开右闭
df_train['bin_age'] = pd.cut(df_train['age'], bins=ages_bins)
df_train['age'].value_counts()
df_train[['age','bin_age']]

# pd.qcut() 等频切分  即每个箱子中含有的数的数量是相同的
使用qcut可以对一组数据分成几个区间 
比如，我们有11家公司，他们的年销售额分别为：
[1000,856,123,523,33,71,223,699,103,456,923]
请你对这11家公司的年销售额进行分箱
1）按照 高/低，两个等级
2）按照 first 10%, second 10%, third 10% 以及 last 70% 四个等级
# 随机销售额
sales = pd.Series([1000,856,123,523,33,71,223,699,103,456,923])
print(len(sales))
# 将销售额分成 低/高 两个等级
print(pd.qcut(sales,[0,0.5,1],labels=['small sales','large sales']))
# 将销售额分成 first 10%, second 10%, third 10% 以及 后70% 四种等级
print(pd.qcut(sales,[0, 0.7, 0.8, 0.9, 1],labels=['last 70%','third 10%','second 10%','first 10%']))

```

### 2. 查看dataframe缺失值、填充

```python
train_data.isnull().any()

null_num = df_train.isnull().sum()
pd.DataFrame({'列名':null_num.index, '缺失值个数':null_num.values, '比例':null_num.values/len(df_train)})
```

填充：

```python
train_data['Age'].fillna(train_data['Age'].mean(),inplace=True)

```





### 3. 特征规范化处理

不处理会导致  后续与距离相关的计算导致不同特征权重不同

使用模型：LR、SVM、KNN、神经网络

不适用的模型：决策树，分裂节点选择是特征x的信息增益率

### 4. 特征组合工具

* 矩阵分解  mf.py

  https://github.com/albertauyeung/matrix-factorization-in-python

  ```python
  from mf import MF
  R = np.array([
      [5,3,0,1],
      [4,0,0,1],
      [1,1,0,5],
      [1,0,0,4],
      [0,1,5,4],
  ])
  mf = MF(R,K=2,alpha=0.1,beta=0.01, iterations=20)
  training_process = mf.train()
  print(mf.P)# 两个隐向量:左齐矩阵、右齐矩阵
  print(mf.Q)
  print(mf.full_matrix)
  ```

* 因子分解机 libfm

  https://github.com/godpgf/pylibfm

  ```python
  from pyfm import pylibfm
  ```

  

* 特征向量化 DictVectorizer

  ```python
  # 对使用字典存储的数据 抽取特征并向量化
  from sklearn.feature_extraction import DictVectorizer
  # 输出转化后的特征矩阵
  vec.fit_transform(measurements).toarray()
  # 输出各个维度的特征名
  vec.get_feature_names()
  
  from sklearn.feature_extraction import DictVectorizer
  train = [
      {'user':'1', 'item':'5', 'age':19},
      {'user':'2', 'item':'43', 'age':33},
      {'user':'3', 'item':'20', 'age':55},
      {'user':'4', 'item':'10', 'age':20},
  ]
  vector = DictVectorizer()
  x_train = vector.fit_transform(train)
  print(x_train)
  print(x_train.toarray())
  ```

  

  

  

### 5. 特征筛选

```python
# XGBoost特征重要性
xgb.plot_importance(model_xgb)
xgb.plot_importance(model_xgb,max_min_features=10,importance_type='gain')
xgb.plot_importance(model_xgb,max_min_features=10,importance_type='cover')
```

* 皮尔逊相关系数

  取值范围[-1,1]

  越接近0=>线性相关性越弱；越接近1或者-1=>线性相关性越强

* dd

### 6. pandas读文件数据

* pandas解析文件 ：自定义解析，关键字参数

  ```python
  # 没有parse_dates参数
  data=pd.read_csv('./user_balance_table.csv')
  data
  '''
  字段report_date显示为：
  20140805
  '''
  
  
  # 有parse_dates参数
  data=pd.read_csv('./user_balance_table.csv',parse_dates=['report_date'])
  data
  '''
  字段report_date显示为：
  2014-08-05
  '''
  ```

  ```python
  # 项目：移动推荐系统
  import datetime
  
  dateparse = lambda dates: datetime.datetime.strptime(dates, '%Y-%m-%d %H')
  data_file = open('./act_34.csv','r')
  # parse_dates：list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column.
  df_act_34 = pd.read_csv(data_file,parse_dates=[0],date_parser=dateparse,index_col=False)
  df_act_34.columns = ['time','user_id', 'item_id', 'behavior_type']
  print(df_act_34.shape)
  
  ```

  

* 加载并提取时间特征

  ```python
  # 项目：资金流入流出预测
  # 数据加载，并提取时间特征
  def data_load():
      data = pd.read_csv('./user_balance_table.csv')
      data = add_timestamp(data)
      return data
  # 给数据添加时间维度
  def add_timestamp(data):
      # 时间维度解析
      data['report_date'] = pd.to_datetime(data['report_date'],format="%Y%m%d")
      #添加时间维度
      data['day'] = data['report_date'].dt.day
      data['month'] = data['report_date'].dt.month
      data['year'] = data['report_date'].dt.year    
      data['week'] = data['report_date'].dt.isocalendar().week   
      data['weekday'] = data['report_date'].dt.weekday
      return data
  
  data = data_load()
  data
  ```

  

* dd

### 7. 一张大图拆成3个小图

### 8. 季节性分解

```python
# 项目：资金流入流出预测

import matplotlib.pyplot as plt
import statsmodels.api as sm
# 指定区间范围的数据可视化
def plot_stl(data):
    # 返回三个部分 trend趋势，seasonal季节，residual残差
    result = sm.tsa.seasonal_decompose(data, period=30)
    # 可视化
    fig = plt.figure(figsize=(20,10))
    ax1 = fig.add_subplot(311)
    ax2 = fig.add_subplot(312)
    ax3 = fig.add_subplot(313)
    
    result.trend.plot(ax=ax1,title='Trend')
    result.seasonal.plot(ax=ax2,title='Seasonal')
    result.resid.plot(ax=ax3,title='Residual')

plot_stl(purchase.total_purchase_amt)   
```

### 9. 平稳性检测（ADF检测）

* 在使用时间序列模型时（比如 ARMA、ARIMA），需要时间序列是平稳的，所以第一步都需要进行平稳性检验，常用的统计检验方法为ADF检验（也称为单位根检验）

* ADF检验，就是判断序列是否存在单位根，如果序列平稳，就不存在单位根，否则，就会存在单位根

* ADF检验的 H0 假设就是存在单位根，如果得到的显著性检验统计量小于三个置信度（10%，5%，1%），则对应有（90%，95，99%）的把握来拒绝原假设

```python
from statsmodels.tsa.stattools import adfuller
t=adfuller(df_p['total_purchase_amt'])
(-1.5898802926313507, 0.4886749751375928, 18, 408, {'1%': -3.446479704252724, '5%': -2.8686500930967354, '10%': -2.5705574627547096}, 15960.28197033403)
输出结果依次为：
t-statistic, p-value, usedlag, nobs
critical-value：测试统计数据的临界值为1%，5%和10%

AIC
如何确定该序列能否平稳：
主要看1%、%5、%10不同程度拒绝原假设的统计值和ADF Test result的比较，如果ADF Test result同时小于1%、5%、10%即说明非常好地拒绝原假设（原假设是不稳定的，因此证明是平稳的）
这里，adf结果为-1.58988， 大于三个level的统计值，无法拒绝
原假设（原假设是不平稳的），需要进行一阶差分后，再进行检验
```

### 10. 生成测试数据并和原数据合并-concat纵向拼接

pd.concat(ignore_index = True) 见项目：天猫复购预测

为true：重新编排索引，否则会有重复的索引值

```python
# 项目：资金流入流出预测
# 生成测试数据
def generate_test_data(data):
    total_balance = data.copy()
    # 生成2014-09-01到2014-09-30的数据
    start = datetime.datetime(2014,9,1)
    end = datetime.datetime(2014,10,1)
    testdata = []
    while start != end:
        # 3个字段，date, total_purchase_amt, total_redeem_amt
        temp = [start, np.nan, np.nan]
        testdata.append(temp)
        # 日期+1
        start += datetime.timedelta(days=1)
    # 封装testdata
    testdata = pd.DataFrame(testdata)
    testdata.columns = total_balance.columns
    # 将testdata合并到total_balance中
    total_balance = pd.concat([total_balance, testdata])
    return total_balance.reset_index(drop=True)

total_balance = generate_test_data(total_balance)
total_balance
```

### 11. pandas的groupby

可以按照两列数据聚合

```python
# 项目：资金流入流出预测

# 分别统计周一到周日在1-31号出现的频次
# as_index=False  day不需要做索引
weekday_count = total_balance[['weekday','day','report_date']]\
                .groupby(['weekday','day'],as_index=False).count()
weekday_count
'''
	weekday	day	report_date
0	0	2	1
1	0	3	1
'''
```

### 12. pandas的DataFrame迭代每一行

```python
# 项目：资金流入流出预测
for index,row in day_pred.iterrows():
    if month_index in (2,4,6,9) and row['day'] == 31:
        break
    # 添加repport_date字段
    day_pred.loc[index,'report_date'] = pd.to_datetime('2014-0'+str(month_index) + '-' + str(int(row['day'])))
day_pred

```

### 13. pandas的使用

* 创建dataframe、series

```python
from pandas import Series, DataFrame
x1 = Series([1,2,3,4])
x2 = Series(data=[1,2,3,4], index=['a', 'b', 'c', 'd'])
# 使用字典来进行创建
d = {'a':1, 'b':2, 'c':3, 'd':4}
x3 = Series(d)
print(x1)
print(x2)
print(x3)
```



```python
from pandas import Series, DataFrame
data = {'Chinese': [66, 95, 93, 90,80], 'Math': [30, 98, 96, 77, 90], 'English': [65, 85, 92, 88, 90]}
df1 = DataFrame(data)
df2 = DataFrame(data, index=['ZhangFei', 'GuanYu', 'LiuBei', 'DianWei', 'XuChu'], columns=['Chinese', 'Math', 'English'])
print(df1)
print(df2)
```

* 删除行、列，重命名列名，去重更改数据格式，删除左右两边空格

  ```python
  df2 = df2.drop(columns=['Chinese']) #删除列
  df2 = df2.drop(index=['ZhangFei']) #删除行
  
  df2.rename(columns={'Chinese': '语文', 'English': 'Yingyu'}, inplace = True)
  
  df = df.drop_duplicates()
  
  df2['Chinese'].astype('str') 
  df2['Chinese'].astype(np.int64) 
  
  df2['Chinese']=df2['Chinese'].map(str.strip)
  ```

  

* 大小写转换

  ```python
  #全部大写
  df2.columns = df2.columns.str.upper()
  #全部小写
  df2.columns = df2.columns.str.lower()
  #首字母大写
  df2.columns = df2.columns.str.title()
  ```

  

* apply函数和map的区别

  * apply 用在dataframe上，用于对row或者column进行计算

  * applymap 用于dataframe上，是元素级别的操作

  * map ，是python自带的，用于series上，是元素级别的操作

  ```python
  df['name'] = df['name'].apply(str.upper)
  
  也可以定义个函数，在apply中进行使用
  def double_df(x):
             return 2*x
  df1[u'语文'] = df1[u'语文'].apply(double_df)
  
  ```

* 统计函数

  count() 统计个数，空值NaN不计算

  describe() 一次性输出多个统计指标，包括：count, mean, std, min, max等

  min()最小值

  max()最大值

  sum()总和

  mean()平均值

  median()中位数

  var()方差

  std()标准差

  argmin() 统计最小值的索引位置

  argmax()统计最大值的索引位置

  idxmin() 统计最小值的索引值

  idxmax() 统计最大值的索引值

  print(df2.describe())

* 数据表合并

  ```python
  df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(1,6)})
  df2 = DataFrame({'name':['ZhangFei', 'GuanYu', 'A', 'B', 'C'], 'data2':range(1,6)})
  1）基于name这列进行连接
  df3 = pd.merge(df1, df2, on='name')
  ```
  
* 索引函数

  * loc函数：通过行索引 "Index" 中的具体值来取行数据（如取"Index"为"A"的行）

  * iloc函数：通过行号来取行数据（如取第二行的数据）

* 聚合函数groupby

  ```python
  import numpy as np
  import pandas as pd
  # 因为文件中有中文，所以采用gbk编码读取
  data = pd.read_csv('heros2.csv', encoding='gbk')
  result = data.groupby('role').agg([np.sum, np.mean])
  print(result)
  ```

* 透视表 pivot unstack

  见项目： 天猫复购预测

  ```python
  Unstack, also known as pivot, Series with MultiIndex to produce DataFrame.
  ```

### 14. dataframe可视化和seaborn、matplotlib可视化

df可以可视化

```python
train_data['Survived'].value_counts().plot(kind='pie',label='Survived')
plt.show()
```



```python
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import os
from matplotlib.font_manager import FontProperties

# 散点图
def scatter():
	# 数据准备
	N = 500
	x = np.random.randn(N)
	y = np.random.randn(N)
	# 用Matplotlib画散点图
	plt.scatter(x, y,marker='x')
	plt.show()
	# 用Seaborn画散点图
	df = pd.DataFrame({'x': x, 'y': y})
	sns.jointplot(x="x", y="y", data=df, kind='scatter');
	plt.show()

# 折线图
def line_chart():
	# 数据准备
	x = [1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910]
	y = [265, 323, 136, 220, 305, 350, 419, 450, 560, 720, 830]
	# 使用Matplotlib画折线图
	plt.plot(x, y)
	plt.show()
	# 使用Seaborn画折线图
	df = pd.DataFrame({'x': x, 'y': y})
	sns.lineplot(x="x", y="y", data=df)
	plt.show()

# 条形图
def bar_chart():
	# 数据准备
	x = ['c1', 'c2', 'c3', 'c4']
	y = [15, 18, 5, 26]
	# 用Matplotlib画条形图
	plt.bar(x, y)
	plt.show()
	# 用Seaborn画条形图
	sns.barplot(x, y)
	plt.show()

# 箱线图
def box_plots():
	# 数据准备
	# 生成0-1之间的20*4维度数据
	data=np.random.normal(size=(10,4)) 
	lables = ['A','B','C','D']
	# 用Matplotlib画箱线图
	plt.boxplot(data,labels=lables)
	plt.show()
	# 用Seaborn画箱线图
	df = pd.DataFrame(data, columns=lables)
	sns.boxplot(data=df)
	plt.show()

# 饼图
def pie_chart():
	# 数据准备
	nums = [25, 33, 37]
	# 射手adc：法师apc：坦克tk
	labels = ['ADC','APC', 'TK']
	# 用Matplotlib画饼图
	plt.pie(x = nums, labels=labels)
	plt.show()

# 饼图
def pie_chart2():
	# 数据准备
	data = {}
	data['ADC'] = 25
	data['APC'] = 33
	data['TK'] = 37
	data = pd.Series(data)
	data.plot(kind = "pie", label='heros')
	plt.show()

# 热力图
def thermodynamic():
	# 数据准备
	np.random.seed(33)
	data = np.random.rand(3, 3)
	heatmap = sns.heatmap(data)
	plt.show()

# 蜘蛛图
def spider_chart():
	# 数据准备
	labels=np.array([u"推进","KDA",u"生存",u"团战",u"发育",u"输出"])
	stats=[76, 58, 67, 97, 86, 58]
	# 画图数据准备，角度、状态值
	angles=np.linspace(0, 2*np.pi, len(labels), endpoint=False)
	stats=np.concatenate((stats,[stats[0]]))
	angles=np.concatenate((angles,[angles[0]]))
	# 用Matplotlib画蜘蛛图
	fig = plt.figure()
	ax = fig.add_subplot(111, polar=True)   
	ax.plot(angles, stats, 'o-', linewidth=2)
	ax.fill(angles, stats, alpha=0.25)
	# 设置中文字体
	font = FontProperties(fname=r"C:\Windows\Fonts\simhei.ttf", size=14)  
	ax.set_thetagrids(angles * 180/np.pi, labels, FontProperties = font)
	plt.show()

# 二元变量分布图
def jointplot():
	# 数据准备
	flights = sns.load_dataset("flights")
	# 用Seaborn画二元变量分布图（散点图，核密度图，Hexbin图）
	sns.jointplot(x="year", y="passengers", data=flights, kind='scatter')
	sns.jointplot(x="year", y="passengers", data=flights, kind='kde')
	sns.jointplot(x="year", y="passengers", data=flights, kind='hex')
	plt.show()

# 成对关系图
def pairplot():
	# 数据准备
	flights = sns.load_dataset('flights')
	# 用Seaborn画成对关系
	sns.pairplot(flights)
	plt.show()

def thermodynamic2():
	flights = sns.load_dataset('flights')
	print(flights)
	flights=flights.pivot('month','year','passengers') #pivot函数重要
	print(flights)
	sns.heatmap(flights) #注意这里是直接传入数据集即可，不需要再单独传入x和y了
	sns.heatmap(flights,linewidth=.5,annot=True,fmt='d')
	plt.show()


# 散点图
#scatter()
# 折线图
#line_chart()
# 条形图
bar_chart()
# 箱线图
#box_plots()
# 饼图
#pie_chart()
#pie_chart2()
# 热力图
#thermodynamic()
#thermodynamic2()
# 蜘蛛图
#spider_chart()
# 二元变量分布图
#jointplot()
# 成对关系图
#pairplot()
```

### 15. 箱线图

画图过程见上

箱盒图共由五个数值点构成，分别是最小观察值（下边缘），25%分位数（Q1），中位数，75%分位数（Q3），最大观察值（上边缘）。

- 中横线：中位数
- IQR：75%分位数（Q3）-25%分位数（Q1）
- 最小观察值（下边缘） = Q1 – 1.5 IQR
- 最大观察值 （上边缘）= Q3 + 1.5 IQR

**特别说明：箱盒图里面的极大值（上边缘值）并非最大值，极小值（下边缘值）也不是最小值。**

如果数据有存在离群点即异常值，他们超出最大或者最小观察值，此时将离群点以“圆点”形式进行展示。

### 16. subplot画分图

(221) (222)

(212)

![image-20210416072050564](%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.assets/subplot分图.png)

```python
def f(t):
    return np.exp(-t) * np.cos(2 * np.pi * t)
t1 = np.arange(0, 5, 0.1)
t2 = np.arange(0, 5, 0.02)
plt.figure()
plt.subplot(221)
plt.plot(t1, f(t1), 'r--')
plt.subplot(222)
plt.plot(t2, np.cos(2 * np.pi * t2), 'r--')
plt.subplot(212)
plt.plot([1, 2, 3, 4], [1, 4, 9, 16])
plt.show()

```

### 17. 相关系数-皮尔逊

```python
import seaborn as sns
plt.figure(figsize=(10,10))
plt.title('Pearson Correlation Between Features')
sbs.heatmap(train_data.corr(),linecolor='white')
plt.show()
```

### 18. 特征向量的重要程度

```python
def train(train_features, train_labels):
	# 构造CART决策树
	clf = DecisionTreeClassifier()
	clf.fit(train_features, train_labels)
	# 显示特征向量的重要程度
	coeffs = clf.feature_importances_
	df_co = pd.DataFrame(coeffs, columns=["importance_"])
	# 下标设置为Feature Name
	df_co.index = train_features.columns
	df_co.sort_values("importance_", ascending=True, inplace=True)
	df_co.importance_.plot(kind="barh")
	plt.title("Feature importance")
	plt.show()
```

### 19. pandas中的onehot编码与labelencoder

* onehot编码最好使用在series中

  get_dummies()

综合20

```python
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
train_data = pd.read_csv('./train.csv')
test_data = pd.read_csv('./test.csv')
print(train_data.describe())
print(train_data.isnull().any())
print(train_data.info())
train_data['Age'].fillna(train_data['Age'].mean(),inplace=True)
test_data['Age'].fillna(test_data['Age'].mean(),inplace=True)
test_data['Fare'].fillna(test_data['Fare'].mean(),inplace=True)
train_data['Embarked'].fillna('S',inplace=True)

def train(train_features, train_labels):
	# 构造CART决策树
	clf = DecisionTreeClassifier()
	clf.fit(train_features, train_labels)
	# 显示特征向量的重要程度
	coeffs = clf.feature_importances_
	df_co = pd.DataFrame(coeffs, columns=["importance_"])
	# 下标设置为Feature Name
	df_co.index = train_features.columns
	df_co.sort_values("importance_", ascending=True, inplace=True)
	df_co.importance_.plot(kind="barh")
	plt.title("Feature importance")
	plt.show()
# =====onehot编码======    
features = ['Pclass','Age','Sex','SibSp','Parch','Fare','Embarked']
train_features= train_data[features]
train_data_hot_encoded = train_features.drop('Sex',axis=1).join(train_features.Sex.str.get_dummies())
train_data_hot_encoded = train_data_hot_encoded.drop('Embarked',axis=1).join(train_data_hot_encoded.Embarked.str.get_dummies())
print(train_data_hot_encoded)
train(train_data_hot_encoded,train_data['Survived'])    
'''
     Pclass        Age  SibSp  Parch     Fare  female  male  C  Q  S
0         3  22.000000      1      0   7.2500       0     1  0  0  1
1         1  38.000000      1      0  71.2833       1     0  1  0  0
2         3  26.000000      0      0   7.9250       1     0  0  0  1
3         1  35.000000      1      0  53.1000       1     0  0  0  1
4         3  35.000000      0      0   8.0500       0     1  0  0  1
..      ...        ...    ...    ...      ...     ...   ... .. .. ..
'''
```

Thinking：pd.factorize()与pd.get_dummies()区别是什么？

pd.factorize()创建一些数字，来表示类别变量，对每一个类别映	射一个ID，这种映射最后只生成一个特征

```python
>>> codes, uniques = pd.factorize(['b', 'b', 'a', 'c', 'b'])
>>> codes
array([0, 0, 1, 2, 0]...)
>>> uniques
array(['b', 'a', 'c'], dtype=object)

```

pd.get_dummies()相当于one-hot编码





分类后每列的数字前可以加prefix前缀

```python
pd.get_dummies(matrix['age_range'])
'''
	0	2	3	4	5	6	7	8
0	1	0	0	0	0	0	0	0
1	0	1	0	0	0	0	0	0
'''
pd.get_dummies(matrix['age_range'], prefix='age')
'''
	age_0	age_2	age_3	age_4	age_5	age_6	age_7	age_8
0		1		0		0		0		0		0		0		0
1		0		1		0		0		0		0		0		0
'''

```



* labelencoder

  0,1,2,3,4,...

### 20. 查看特征向量的重要程度

综合19

### 21. 可视化工具

powerbi

flask

### 22. 矩阵稀疏性计算

```python
n_users = len(clean_df.UserID.unique())
n_games = len(clean_df.Game.unique())
print('数据集中包含了 {0} 玩家，{1} 游戏'.format(n_users, n_games))

# 矩阵的稀疏性
sparsity = clean_df.shape[0] / float(n_users * n_games)
print('用户行为矩阵的稀疏性（填充比例）为{:.2%} '.format(sparsity))
```





### 23. 回归分析

fit(X,y)，训练，拟合参数

predict(X) ，预测

coef_ ，存放回归系数

intercept_，存放截距

score(X,y)， 得到评分结果，R方（确定系数），**越高越好**

**R方（r-squared）**：

R方也叫确定系数（coefficient of determination），表示模型对现实数据拟合的程度，评估预测效果

R方计算，等于1减去y对回归方程的方差（未解释离差）与y的总方差的比值

一元线性回归中R方等于皮尔逊积矩相关系

比如，R平方=0.8，表示回归关系可以解释因变量80%的变异。换句话说，如果我们能控制自变量x不变，那么因变量y的变异程度会减少80%

在sklearn计算中，相关系数有正负

```python
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

print('一元线性回归')
x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
y = np.array([5, 20, 14, 32, 22, 38])
print(x)
model = LinearRegression()
model.fit(x,y)
r_sq = model.score(x,y)
print('评分结果：',r_sq)
print('系数：',model.coef_)
print('截距：',model.intercept_)

print('多元线性回归')
x = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]]
y = [4, 5, 20, 14, 32, 22, 38, 43]
x,y = np.array(x), np.array(y)
model = LinearRegression()
model.fit(x,y)
r_sq = model.score(x,y)
print('评分结果：',r_sq)
print('系数：',model.coef_)
print('截距：',model.intercept_)

print('多项式回归')
x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
y = np.array([15, 11, 2, 8, 25, 32])
transformer=PolynomialFeatures(degree=2,include_bias=False)
x_ = transformer.fit_transform(x)
print(x_)
model = LinearRegression()
model.fit(x,y)
r_sq = model.score(x,y)
print('评分结果：',r_sq)
print('系数：',model.coef_)
print('截距：',model.intercept_)
```

### 24. defaultdict使用的注意事项

```python
from collections import defaultdict
dict1 = defaultdict(int)# 括号内没有东西会报错
dict2 = defaultdict(set)
dict3 = defaultdict(str)
dict4 = defaultdict(list)
# dict1 = defaultdict()
# dict2 = defaultdict(set)
# dict3 = defaultdict(str)
# dict4 = defaultdict(list)
print(dict1[1])
print(dict2[1])
print(dict3[1])
print(dict4[1])

```

### 25. pandas编码

```python
with open('./Assignment/chinese_stopwords.txt','r',encoding='utf-8') as file:
    # 直接file.readlines()得到的元素最后是换行符，用切片去掉后面的换行符
    stopwords = [line[:-1] for line in file.readlines()]
    
    
# gb18030编码收录的中文字符更全，对中文文档兼容性更好
df = pd.read_csv('./Assignment/sqlResult.csv',encoding='gb18030')
df.info()
```

### 26. 生成0-1标签label

```python
# 项目：文本抄袭自动检测

# 标记是否是某家报社（以新华社为例）的新闻
label = df['source'].copy()#When ``deep=True`` (default), a new object will be created with a copy of the calling object's data and indices
target = label.str.contains('新华') * 1# .contain()返回bool，*1之后bool->0,1
# 确认下目标值无缺失值
target.isnull().sum()
```



### 27. 做特征工程的工具

featuretools

### 28. 数据质量的准则

好的数据质量，应该满足“完全合一”

**完整性**：数据是否存在空值，字段是否完善，是否有漏掉

**全面性**：观察某一列的全部数值及特征值，是否存在单位、字段名与数值不匹配

**合法性**：数据的类型、内容、大小的合法性。

**唯一性**：数据是否存在重复记录

* 问题1：缺失值

  在数据中有些年龄、体重数值是缺失的，这往往是因为数据量较大，在过程中，有些数值没有采集到。通常我们可以采用以下三种方法：

  删除：删除数据缺失的记录；

  均值：使用当前列的均值；

  高频：使用当前列出现频率最高的数据。

  比如我们想对df['Age']中缺失的数值用平均年龄进行填充，可以这样写：

  df['Age'].fillna(df['Age'].mean(), inplace=True)

* 问题2：空行

  我们发现数据中有一个空行，除了 index 之外，全部的值都是 NaN。Pandas 的 read_csv() 并没有可选参数来忽略空行，这样，我们就需要在数据被读入之后再使用 dropna() 进行处理，删除空行。

  \# 删除全空的行

  df.dropna(how='all',inplace=True) 

* 问题3：列数据的单位不统一

  weight列的数值，有的单位是千克（kgs），有的单位是磅（lbs）。

  这里统一将磅（lbs）转化为千克（kgs）：

  \# 获取 weight 数据列中单位为 lbs 的数据

  rows_with_lbs = df['weight'].str.contains('lbs').fillna(False)

  \# 将 lbs转换为 kgs, 2.2lbs=1kgs

  for i,lbs_row in df[rows_with_lbs].iterrows():

   \# 截取从头开始到倒数第三个字符之前，即去掉lbs。

   weight = int(float(lbs_row['weight'][:-3])/2.2)

   df.at[i,'weight'] = '{}kgs'.format(weight) 

* 问题4：非ASCII字符

  如果文本中存在非 ASCII 的字符。我们还需要进行删除或者替换。

  这里使用对非ASCII字符进行删除方式

  \# 删除非 ASCII 字符

  `df['name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)`

* 问题5：一列有多个参数（可选）

  可以将Name分成last name + first name

  也可以进行保留。

  \# 切分名字，删除源数据列

  df[['first_name','last_name']] = df['name'].str.split(expand=True)

  df.drop('name', axis=1, inplace=True)

  默认采用的空格进行分割，相当于df['name'].str.split(' ', expand=True)

* 问题6：重复数据

  我们校验一下数据中是否存在重复记录。如果存在重复记录，就使用 Pandas 提供的 drop_duplicates() 来删除重复数据。

  \# 删除重复数据行

  df.drop_duplicates(['first_name','last_name'],inplace=True)

### 29. 特征向量化dictvectorizer

```python
from sklearn.feature_extraction import DictVectorizer
 
dict_vec = DictVectorizer(sparse=False)# #sparse=False意思是不产生稀疏矩阵
 
X_train = dict_vec.fit_transform(X_train.to_dict(orient='record'))
X_test = dict_vec.transform(X_test.to_dict(orient='record'))
print(dict_vec.feature_names_)#查看转换后的列名
print(X_train)#查看转换后的训练集
['age','pclass=1st', 'pclass=2nd', 'pclass=3rd', 'sex=female', 'sex=male']
[[31.19418104  0.          0.          1.          0.          1.        ]
 [31.19418104  1.          0.          0.          1.          0.        ]
 [31.19418104  0.          0.          1.          0.          1.        ]
 ...
 [12.          0.          1.          0.          1.          0.        ]
 [18.          0.          1.          0.          0.          1.        ]
 [31.19418104  0.          0.          1.          1.          0.        ]]

原pclass和sex列如下：
full[['Pclass','Sex']].head()
	Pclass	Sex
0	3	male
1	1	female
2	3	female
3	1	female
4	3	male
```

### 30. pandas 数据类型转换使用astype

### 31. dataframe排序sort_values可以设置多标准

```python
df.sort_values(['UserID', 'Game', 'Hours_Played'], ascending=True)
先按userid排，相同的按game排，再相同按hours_played排
```

### 32. dataframe删除重复项drop_duplicates可以选择

```python
# 注意keep
# 删除重复项，并保留最后一项出现的项（因为最后一项是用户游戏时间，第一项为购买）
clean_df = df.drop_duplicates(['UserID', 'Game'], keep = 'last')
# 去掉不用的列：Action, Hours, Not Needed
clean_df = clean_df.drop(['Action', 'Hours', 'Not Needed'], axis = 1)

```

### 31. dataframe删除列drop([待删的列],axis=1)

### 32. KMeans手肘取k法

曲率/弯曲率最大的点

```python
# 统计不同K取值的误差平方和
sse = []
for k in range(1, 11):
	# kmeans算法
	kmeans = KMeans(n_clusters=k)
	kmeans.fit(train_x)
	# 计算inertia簇内误差平方和
	sse.append(kmeans.inertia_)
x = range(1, 11)
plt.xlabel('K')
plt.ylabel('SSE')
plt.plot(x, sse, 'o-')
plt.show()
```

### 33. plt画图图中显示中文

plt.rcParams['font.sans-serif']=['SimHei']

不写上行，中文显示不了

```python
plt.rcParams['font.sans-serif']=['SimHei']

plt.figure(figsize=(6,6))
plt.title('信用卡违约率客户\n (违约：1，守约：0)')
sns.set_color_codes('pastel')
sns.barplot(x='default.payment.next.month',y='values',data=df)
plt.show()

```

### 34. GridSearchCV参数调优

```python
# -*- coding: utf-8 -*-
# 信用卡违约率分析
import pandas as pd
from sklearn.model_selection import learning_curve, train_test_split,GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from matplotlib import pyplot as plt
import seaborn as sns
# 数据加载
data = data = pd.read_csv('./UCI_Credit_Card.csv')
# 数据探索
print(data.shape) # 查看数据集大小
print(data.describe()) # 数据集概览
# 查看下一个月违约率的情况
next_month = data['default.payment.next.month'].value_counts()
print(next_month)
df = pd.DataFrame({'default.payment.next.month': next_month.index,'values': next_month.values})
plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签
plt.figure(figsize = (6,6))
plt.title('信用卡违约率客户\n (违约：1，守约：0)')
sns.set_color_codes("pastel")
sns.barplot(x = 'default.payment.next.month', y="values", data=df)
plt.show()
# 特征选择，去掉ID字段、最后一个结果字段即可
data.drop(['ID'], inplace=True, axis =1) #ID这个字段没有用
target = data['default.payment.next.month'].values
columns = data.columns.tolist()
columns.remove('default.payment.next.month')
features = data[columns].values
# 30%作为测试集，其余作为训练集
train_x, test_x, train_y, test_y = train_test_split(features, target, test_size=0.30, stratify = target, random_state = 1)
    
# 构造各种分类器
classifiers = [
    SVC(random_state = 1, kernel = 'rbf'),    
    DecisionTreeClassifier(random_state = 1, criterion = 'gini'),
    RandomForestClassifier(random_state = 1, criterion = 'gini'),
    KNeighborsClassifier(metric = 'minkowski'),
]
# 分类器名称
classifier_names = [
            'svc', 
            'decisiontreeclassifier',
            'randomforestclassifier',
            'kneighborsclassifier',
]
# 分类器参数
classifier_param_grid = [
            {'svc__C':[1], 'svc__gamma':[0.01]},
            {'decisiontreeclassifier__max_depth':[6,9,11]},
            {'randomforestclassifier__n_estimators':[3,5,6]} ,
            {'kneighborsclassifier__n_neighbors':[4,6,8]},
]
 
# 对具体的分类器进行GridSearchCV参数调优
def GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, param_grid, score = 'accuracy'):
    response = {}
    gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, scoring = score)
    # 寻找最优的参数 和最优的准确率分数
    search = gridsearch.fit(train_x, train_y)
    print("GridSearch最优参数：", search.best_params_)
    print("GridSearch最优分数： %0.4lf" %search.best_score_)
    predict_y = gridsearch.predict(test_x)
    print("准确率 %0.4lf" %accuracy_score(test_y, predict_y))
    response['predict_y'] = predict_y
    response['accuracy_score'] = accuracy_score(test_y,predict_y)
    return response
 
for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid):
    pipeline = Pipeline([
            ('scaler', StandardScaler()),
            (model_name, model)
    ])
    result = GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, model_param_grid , score = 'accuracy')

```



### 35. dataframe取一行分别得到df和series

```python
df[df.index==0]
type(df[df.index==0])
pandas.core.frame.DataFrame

df.iloc[0]
type(df.iloc[0])
pandas.core.series.Series
```

### 36. dataframe和series的.values结果不同

```python
df[df.index==0].values
array([[...]])

df.iloc[0].values
array([..])
```

### 37. N-Gram

什么是N-Gram（N元语法）：

* 基于一个假设：第n个词出现与前n-1个词相关，而与其他任何词不相关.

* N=1时为unigram，N=2为bigram，N=3为trigram

* N-Gram指的是给定一段文本，其中的N个item的序列

比如文本：A B C D E，对应的Bi-Gram为A B, B C, C D, D E

* 当一阶特征不够用时，可以用N-Gram做为新的特征。比如在处理文本特征时，一个关键词是一个特征，但有些情况不够用，需要提取更多的特征，采用N-Gram => 可以理解是相邻两个关键词的特征组合

### 38. 余弦相似度的计算

* 使用linear_kernel

http://lijiancheng0614.github.io/scikit-learn/modules/metrics.html#linear-kernel

`cosine_similarity` accepts `scipy.sparse` matrices. (Note that the tf-idf functionality in `sklearn.feature_extraction.text` can produce normalized vectors, in which case `cosine_similarity` is equivalent to [`linear_kernel`](http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.metrics.pairwise.linear_kernel.html#sklearn.metrics.pairwise.linear_kernel), only slower.)

```python
# 实战：为酒店建立内容推荐系统

from sklearn.metrics.pairwise import linear_kernel
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_matrix = tf.fit_transform(df['desc_clean'])
print('TFIDF feature names:')
#print(tf.get_feature_names())
print(len(tf.get_feature_names()))
#print('tfidf_matrix:')
#print(tfidf_matrix)
#print(tfidf_matrix.shape)
# 计算酒店之间的余弦相似度（线性核函数）
cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)
#print(cosine_similarities)
#print(cosine_similarities.shape)
indices = pd.Series(df.index) #df.index是酒店名称
```

### 39. 计算词频CountVectorizer

```python
# 实战：为酒店建立内容推荐系统

# 得到酒店描述中n-gram特征中的TopK个
def get_top_n_words(corpus, n=1, k=None):
    # 统计ngram词频矩阵
    vec = CountVectorizer(ngram_range=(n, n), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    print(bag_of_words)
#     print(type(bag_of_words))
    sum_words = bag_of_words.sum(axis=0)
    print(type(sum_words))
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    # 按照词频从大到小排序
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:k]
common_words = get_top_n_words(df['desc'], 3, 20)
print(common_words)
```

### 40. 计算tfidf

* sklearn.feature_extraction.text

```python
import jieba
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer
from sklearn.metrics.pairwise import cosine_similarity

wordslist = ["我非常非常喜欢看电视剧","我非常喜欢旅行","我非常喜欢吃苹果", '我非常喜欢跑步','王者荣耀KPL春季赛开战啦'] 

# 王者荣耀 被拆开了，不合适，因此加
jieba.add_word('王者荣耀')
textTest = [' '.join(jieba.cut(words)) for words in wordslist]
print(textTest)

'''
第一种方法：文本->先CountVectorizer()统计词频->TfidfTransformer()
'''
# 统计词频
vectorizer = CountVectorizer()
count = vectorizer.fit_transform(textTest)
print(count)
# 输出特征
print(vectorizer.get_feature_names())
# 输出word与id
print(vectorizer.vocabulary_)
# 输出词频
print(count.toarray())

# 计算tfidf
transformer = TfidfTransformer()
tfidf_matrix = transformer.fit_transform(count)
print(tfidf_matrix.toarray())

'''
第二种方法：文本->TfidfVectorizer()
'''
tfidf_vec = TfidfVectorizer()
tfidf_matrix = tfidf_vec.fit_transform(textTest)
print(tfidf_matrix.toarray())

# 看句子之间的相似度
print('cosine_similarity=\n',cosine_similarity(tfidf_matrix, tfidf_matrix))
```

* gensim

  ```python
  # Gensim, TF-IDF使用
  import jieba
  from gensim import corpora, models, similarities
  
  # 加载数据
  wordslist = ["我非常喜欢看电视剧","我非常喜欢旅行","我非常喜欢吃苹果", '我非常喜欢跑步'] 
  # 分词
  textTest = [[word for word in jieba.cut(words)] for words in wordslist]
  print('分词：', textTest)
  # 生成字典
  dictionary = corpora.Dictionary(textTest)
  print('字典：', dictionary)
  print('单词词频:', dictionary.dfs)  # 字典词频，{单词id，在多少文档中出现}
  print('文档数目:', dictionary.num_docs)  # 文档数目
  print('所有词的个数:', dictionary.num_pos)  # 所有词的个数
  featurenum = len(dictionary.token2id.keys())
  print('featurenum', featurenum)
  
  # 生成语料 
  corpus = [dictionary.doc2bow(text) for text in textTest]
  print('语料：', corpus)
  
  """ 计算语料的TFIDF """ 
  # 训练TFIDF模型 
  tfidf_model = models.TfidfModel(corpus, dictionary=dictionary)
  print('tfidf_model=', tfidf_model) # 只要记录BOW矩阵的非零元素个数(num_nnz)
  
  # 得到语料的TFIDF值
  corpus_tfidf = tfidf_model[corpus]
  print('corpus=', corpus)
  print('转换整个语料库：')
  for doc in corpus_tfidf:
      print(doc)
  
  # 生成余弦相似度索引, 使用SparseMatrixSimilarity()，可以占用更少的内存和磁盘空间。
  index = similarities.SparseMatrixSimilarity(corpus_tfidf, num_features=featurenum) 
  
  
  
  """ 对于新的句子，生成 BOW向量，与之前的句子计算相似度 """
  test = jieba.lcut("我喜欢看电视剧")
  print('分词', test)
  # 生成BOW向量
  vec = dictionary.doc2bow(test)
  print('BOW向量', vec)
  # 计算tfidf向量
  test_vec = tfidf_model[vec]
  print('test vec=', test_vec)
  # 返回test_vec 和训练语料中所有文本的余弦相似度。返回结果是个numpy数组
  print(index.get_similarities(test_vec))
  
  
  
  ```

  



```python
# 实战：为酒店建立内容推荐系统

from sklearn.feature_extraction.text import TfidfVectorizer

# 使用TF-IDF提取文本特征
tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0.01, stop_words='english')

tfidf_matrix = tf.fit_transform(df['desc_clean'])
```





### 41. 在一个series中查找某个内容，并返回该内容对应的索引

```python
indices = pd.Series(df.index)
idx = indices[indices == name].index[0]
```

### 42. word2vec工具-gensim

* `model.wv.most_similar(positive=['孙悟空', '唐僧'], negative=['孙行者'])`

  表示：与`'孙悟空' +  '唐僧' - '孙行者'`最相似的词

* 查看某个词 训练完成后的词向量：`model.wv['某个词']`

```python
# 实战：word2vec

# -*-coding: utf-8 -*-
# 先运行 word_seg进行中文分词，然后再进行word_similarity计算
# 将Word转换成Vec，然后计算相似度 
from gensim.models import word2vec
import multiprocessing

# 如果目录中有多个文件，可以使用PathLineSentences
segment_folder = './journey_to_the_west/segment'
sentences = word2vec.PathLineSentences(segment_folder)

# 设置模型参数，进行训练
model = word2vec.Word2Vec(sentences, size=100, window=3, min_count=1)
print(model.wv.similarity('孙悟空', '猪八戒'))
print(model.wv.similarity('孙悟空', '孙行者'))
print(model.wv.most_similar(positive=['孙悟空', '唐僧'], negative=['孙行者']))
# 设置模型参数，进行训练
model2 = word2vec.Word2Vec(sentences, size=128, window=5, min_count=5, workers=multiprocessing.cpu_count())
# 保存模型
model2.save('./models/word2Vec.model')
print(model2.wv.similarity('孙悟空', '猪八戒'))
print(model2.wv.similarity('孙悟空', '孙行者'))
print(model2.wv.most_similar(positive=['孙悟空', '唐僧'], negative=['孙行者']))

print(model2.wv['孙悟空'])
```





### 43. read()/readline()/readlines()

`read() `每次读取整个文件，它通常将读取到底文件内容放到一个字符串变量中，也就是说 .read() 生成文件内容是一个**字符串类型**。
`readline()`每只读取文件的一行，通常也是读取到的一行内容放到一个字符串变量中，**返回str类型**。
`readlines()`每次按行读取整个文件内容，将读取到的内容放到一个列表中，**返回list类型**。



### 45. google colab云编辑器

### 46. 推荐系统工具surprise：svd 、baseline、slopeone、协同过滤

* baseline

![image-20210515164216599](%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.assets/image-20210515164216599.png)



als 和sgd两种方法

```python
from surprise import Reader
from surprise import BaselineOnly, KNNBasic
from surprise import accuracy
from surprise.model_selection import KFold
# 数据读取
reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)
data = Dataset.load_from_file('./ratings.csv', reader=reader)
train_set = data.build_full_trainset()
# Baseline算法，使用ALS进行优化
'''
ALS参数:
reg_i：物品的正则化参数，默认为10。
reg_u：用户的正则化参数，默认为15 。
n_epochs：迭代次数，默认为10

'''
bsl_options = {'method': 'als','n_epochs': 5,'reg_u': 12,'reg_i': 5}
'''
# Baseline算法，使用ALS进行优化
bsl_options = {'method': 'als','n_epochs': 5,'reg_u': 12,'reg_i': 5}
SGD参数:
reg：代价函数的正则化项，默认为0.02。
learning_rate：学习率，默认为0.005。
n_epochs：迭代次数，默认为20。
'''
algo = BaselineOnly(bsl_options=bsl_options)
# 定义K折交叉验证迭代器，K=3
kf = KFold(n_splits=3)
for trainset, testset in kf.split(data):
    algo.fit(trainset)
    predictions = algo.test(testset)
    accuracy.rmse(predictions, verbose=True)
uid = str(196)
iid = str(302)
pred = algo.predict(uid, iid, r_ui=4, verbose=True)
```



* CF 基于KNN的协同过滤

  https://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html

  这面四种的区别：相似用户们个个的打分-某个值x

  x=0，basic；x=用户个个的平均兴趣，withmeans；个个打分的标准化，withzscore；x=baseline(=全局平均分+该用户的评分偏置+该item的评分偏置)，baseline

  knn中的k是指 选择前k个最相似的用户

  * knns.KNNBasic

    ![image-20210515162118163](%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.assets/image-20210515162118163.png)

  * knns.KNNWithMeans

    ![image-20210515162155332](%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.assets/image-20210515162155332.png)
  
  * knns.KNNWithZScore
  
    ![image-20210515162244425](%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.assets/image-20210515162244425.png)
  
  * knns.KNNBaseline
  
    ![image-20210515162415842](%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.assets/image-20210515162415842.png)
  
  **k经验值取20-50之间**

```python


from surprise import KNNWithMeans
from surprise import Dataset, Reader

# 数据读取
reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)
data = Dataset.load_from_file('./ratings.csv', reader=reader)
trainset = data.build_full_trainset()

# ItemCF 计算得分
# 取最相似的用户计算时，只取最相似的k个
algo = KNNWithMeans(k=50, sim_options={'user_based': False, 'verbose': 'True'})
algo.fit(trainset)

uid = str(196)
iid = str(302)

pred = algo.predict(uid, iid)
print(pred)
```



* slopeone

  * 常规版实例：

    |      | **a** | **b** | **c** | **d** |
    | ---- | ----- | ----- | ----- | ----- |
    | A    | 5     | 3.5   | ？    | ？    |
    | B    | 2     | 5     | 4     | 2     |
    | C    | 4.5   | 3.5   | 1     | 4     |
  
    * Step1，计算Item之间的评分差的均值

      b与a：((3.5-5)+(5-2)+(3.5-4.5))/3=0.5/3

      c与a：((4-2)+(1-4.5))/2=-1.5/2

      d与a：((2-2)+(4-4.5))/2=-0.5/2

      c与b：((4-5)+(1-3.5))/2=-3.5/2

      d与b：((2-5)+(4-3.5))/2=-2.5/2

      d与c：((2-4)+(4-1))/2=1/2

    * Step2，预测用户A对商品c和d的评分

      a对c评分=((-0.75+5)+(-1.75+3.5))/2=3

      a对d评分=((-0.25+5)+(-1.25+3.5))/2=3.5

    * Step3，将预测评分排序，推荐给用户

      推荐顺序为{d, c}

  * 改进版实例-加权

    https://blog.csdn.net/xidianliutingting/article/details/51916578

    假设有100个人对物品A和物品B打分了，R(AB)表示这100个人对A和B打分的平均偏差;有1000个人对物品B和物品C打分了， R(CB)表示这1000个人对C和B打分的平均偏差；

    ![image-20210515173505446](%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.assets/image-20210515173505446.png)

    * SlopeOne算法的特点：

      * 适用于item更新不频繁，数量相对较稳定

      * item数<<user数

      * 算法简单，易于实现，执行效率高

      * 依赖用户行为，存在冷启动问题和稀疏性问题

```python
# 使用SlopeOne算法
algo = SlopeOne()
algo.fit(train_set)
# 对指定用户和商品进行评分预测
uid = str(196) 
iid = str(302) 
pred = algo.predict(uid, iid, r_ui=4, verbose=True)
```



* SVD

  **funkSVD**: p、q

  **biasSVD**: p q bi bu

  **SVD++**：p q bi bu +隐型行为

  第一个可以用ALS，其余使用SGD

  工具：

  * `class surprise.prediction_algorithms.matrix_factorization.SVD` 

    biasSVD算法

    使用Surprise工具中的SVD

    参数:

    n_factors: k值，默认为100

    n_epochs：迭代次数，默认为20

    **biased：是否使用biasSVD，默认为True；False就是funkSVD**

    verbose:输出当前epoch，默认为False

    reg_all:所有正则化项的统一参数，默认为0.02

    reg_bu：bu的正则化参数，reg_bi：bi的正则化参数

    reg_pu：pu的正则化参数，reg_qi：qi的正则化参数

  * `class surprise.prediction_algorithms.matrix_factorization.SVDpp` 

    SVD++算法

    使用Surprise工具中的SVDpp

    参数:

    n_factors: k值，默认为20

    n_epochs：迭代次数，默认为20

    verbose:输出当前epoch，默认为False

    reg_all:所有正则化项的统一参数，默认为0.02

    reg_bu：bu的正则化参数，reg_bi：bi的正则化参数

    reg_pu：pu的正则化参数，reg_qi：qi的正则化参数

    reg_yj：yj的正则化参数

```python
# 实战：movelens电影推荐

#!pip install surprise
from surprise import SVD
from surprise import Dataset
from surprise.model_selection import cross_validate
from surprise import Reader
from surprise import accuracy
from surprise.model_selection import KFold
import pandas as pd
import time

"""
from google.colab import drive
drive.mount('/content/drive')
import os
os.chdir("/content/drive/My Drive/Colab Notebooks/")
"""
time1=time.time()

# 数据读取
reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)
data = Dataset.load_from_file('ratings.csv', reader=reader)
train_set = data.build_full_trainset()

# 使用funkSVD
algo = SVD(biased=False)

# 定义K折交叉验证迭代器，K=3
kf = KFold(n_splits=3)
for trainset, testset in kf.split(data):
    # 训练并预测
    algo.fit(trainset)
    predictions = algo.test(testset)
    # 计算RMSE
    accuracy.rmse(predictions, verbose=True)

uid = str(196)
iid = str(302)
# 输出uid对iid的预测结果
pred = algo.predict(uid, iid, r_ui=4, verbose=True)
time2=time.time()
print(time2-time1)

```

### 47.  DeepCTR工具：deepfm、NFM、din使用

* deepfm = dnn + fm：稀疏特征-》紧密特征-》一路fm，一路dnn，紧密特征两路都走-》sigmoid函数-》输出
  * 神经元个数：每层的神经元个数越多，未必越好；设置为200-400个比较适合
  * 隐藏层层数：层数越多，起初效果明显，之后效果不明显，甚至更差，容易过拟合；3层比较适合
  * 网络形状：固定(200-200-200),增长(100-200-300), 下降(300-200-100), 菱形(150-300-150)；采用固定的形状效果最好
* din中的VarLenSparseFeat很可能与序列特征有关

deepfm auc

https://www.cnblogs.com/xiaoqi/p/deepfm.html

```python
# 实战：movielens电影推荐

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from deepctr.models import DeepFM
from deepctr.feature_column import SparseFeat,get_feature_names

#数据加载
data = pd.read_csv("movielens_sample.txt")
sparse_features = ["movie_id", "user_id", "gender", "age", "occupation", "zip"]
target = ['rating']

# 对特征标签进行编码
for feature in sparse_features:
    lbe = LabelEncoder()
    data[feature] = lbe.fit_transform(data[feature])
# 计算每个特征中的 不同特征值的个数
fixlen_feature_columns = [SparseFeat(feature, data[feature].nunique()) for feature in sparse_features]
linear_feature_columns = fixlen_feature_columns
dnn_feature_columns = fixlen_feature_columns
feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)

# 将数据集切分成训练集和测试集
train, test = train_test_split(data, test_size=0.2)
train_model_input = {name:train[name].values for name in feature_names}
test_model_input = {name:test[name].values for name in feature_names}

# 使用DeepFM进行训练
model = DeepFM(linear_feature_columns, dnn_feature_columns, task='regression')
model.compile("adam", "mse", metrics=['mse'], )
history = model.fit(train_model_input, train[target].values, batch_size=256, epochs=1, verbose=True, validation_split=0.2, )
# 使用DeepFM进行预测
pred_ans = model.predict(test_model_input, batch_size=256)
# 输出RMSE或MSE
mse = round(mean_squared_error(test[target].values, pred_ans), 4)
rmse = mse ** 0.5
print("test RMSE", rmse)
```

### 48. 分类特征编码labelencoder

```python
# 项目：员工离职预测

# 对于分类特征进行特征值编码
from sklearn.preprocessing import LabelEncoder
attr=['Age','BusinessTravel','Department','Education','EducationField','Gender','JobRole','MaritalStatus','Over18','OverTime']
for feature in attr:
    lbe=LabelEncoder()
    train[feature]=lbe.fit_transform(train[feature])
    test[feature]=lbe.transform(test[feature])
train.to_csv('temp.csv')


```

### 49. SVM工具

* svc分类，svr回归

* `sklearn.svm.SVC` 和 `sklearn.svm.NuSVC`没有区别

```python
sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)　

sklearn.svm.NuSVC(nu=0.5, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None) 

sklearn.svm.LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)
```

常用参数：

•C，惩罚系数，类似于LR中的正则化系数，C越大惩罚越大

•nu，代表训练集训练的错误率的上限（用于NuSVC）

•kernel，核函数类型，RBF, Linear, Poly, Sigmoid，precomputed，默认为RBF径向基核（高斯核函数）

•gamma，核函数系数，默认为auto

•degree，当指定kernel为'poly'时，表示选择的多项式的最高次数，默认为三次多项式

•probability，是否使用概率估计

•shrinking，是否进行启发式，SVM只用少量训练样本进行计算

•penalty，正则化参数，L1和L2两种参数可选，仅LinearSVC有

•loss，损失函数，有‘hinge’和‘squared_hinge’两种可选，前者又称L1损失，后者称为L2损失

tol: 残差收敛条件，默认是0.0001，与LR中的一致

### 50. Xlearn工具：FM/FFM/LR模型

### 51. XGBoost工具

可以设置早停法

```python
# 天猫用户复购预测（XGBoost使用示意）
X_train, X_valid, y_train, y_valid = train_test_split(train_X, train_y, test_size=.2)
# 使用XGBoost
model = xgb.XGBClassifier(
    max_depth=8, #树的最大深度
    n_estimators=1000, #提升迭代的次数，也就是生成多少基模型
    min_child_weight=300, #一个子集的所有观察值的最小权重和
    colsample_bytree=0.8, #列采样率，也就是特征采样率
    subsample=0.8, #构建每棵树对样本的采样率
    eta=0.3,    # eta通过缩减特征的权重使提升计算过程更加保守，防止过拟合
    seed=42    #随机数种子
)

model.fit(
    X_train, y_train,
    eval_metric='auc', eval_set=[(X_train, y_train), (X_valid, y_valid)],
    verbose=True,
    #早停法，如果auc在10epoch没有进步就stop
    early_stopping_rounds=10 
)
model.fit(X_train, y_train)
prob = model.predict_proba(test_data)

```

### 52. LightGBM工具

```python
import lightgbm as lgb

param = {'boosting_type':'gbdt',
                         'objective' : 'binary', #任务类型
                         'metric' : 'auc', #评估指标
                         'learning_rate' : 0.01, #学习率
                         'max_depth' : 15, #树的最大深度
                         'feature_fraction':0.8, #设置在每次迭代中使用特征的比例
                         'bagging_fraction': 0.9, #样本采样比例
                         'bagging_freq': 8, #bagging的次数
                         'lambda_l1': 0.6, #L1正则
                         'lambda_l2': 0, #L2正则
        }


X_train, X_valid, y_train, y_valid = train_test_split(train.drop('Attrition',axis=1), train['Attrition'], test_size=0.2, random_state=42)
trn_data = lgb.Dataset(X_train, label=y_train)
val_data = lgb.Dataset(X_valid, label=y_valid)
model = lgb.train(param,train_data,valid_sets=[train_data,valid_data],num_boost_round = 10000 ,early_stopping_rounds=200,verbose_eval=25, categorical_feature=attr)
predict=model.predict(test)
test['Attrition']=predict
# 转化为二分类输出
test['Attrition']=test['Attrition'].map(lambda x:1 if x>=0.5 else 0)
test[['Attrition']].to_csv('submit_lgb.csv')

```

### 53. 神经网络输入的数据最好标准化处理

### 53. 神经网络中的优化方法使用最多的是adam

### 55. tensorflow中的unsqueeze和squeeze

| unsqueeze() | 升维 | 在指定位置上增加维度，并指定维度为1                       |
| ----------- | ---- | --------------------------------------------------------- |
| squeeze()   | 降维 | 删除指定维度，如果该维度为1，则删除成功，否则数据保持不变 |

### 56. 数据处理会用到的创建空字典

```python
from collections import defaultdict
dict1 = defaultdict(int)
dict2 = defaultdict(set)
dict3 = defaultdict(str)
dict4 = defaultdict(list)
print(dict1[1])
print(dict2[1])
print(dict3[1])
print(dict4[1])
```

### 57. python时间处理

```python
import time
timestamp = time.time()
print("当前时间戳为:", timestamp)
localtime = time.localtime(timestamp)
print("本地时间为 :", localtime)

本地时间为 : 
time.struct_time(tm_year=2020, tm_mon=3, tm_mday=19, tm_hour=21, tm_min=42, tm_sec=24, tm_wday=3, tm_yday=79, tm_isdst=0)

```

### 58. 将大文件截取出小文件with open-读块

* 第一种：

```python
# 项目：移动推荐系统

max_size = 100000
size = 0
with open('./sample.csv','wb') as writer:
    with open('./tianchi_fresh_comp_train_user.csv','rb') as file:
        while size <= max_size:
            size += 1
            line = file.readline()
            writer.write(line)

```

* 第二种

  ```python
  # 项目：移动推荐系统
  
  batch=0
  dateparse = lambda dates: pd.datetime.strptime(dates, '%Y-%m-%d %H')
  #date_parser = dateparse ,parse_dates=['time']
  for df in pd.read_csv(open('./tianchi_fresh_comp_train_user.csv', 'r'),chunksize = 100000):
  #     print(df.head())
  #     print(df.columns)
      df_act_34 = df[df['behavior_type'].isin([3,4])]
      df_act_34.to_csv('./act_34.csv',columns=['time','user_id', 'item_id', 'behavior_type'],index=False,header=False,mode='a')
      batch += 1
      print('chunk %d done'%batch)
  
  ```

  

### 59. pandas设置最大显示列数

```python
pd.options.display.max_columns = 100
```

### 60. dataframe中的time字段设置为pandas里面的datetime类型

```python
pd.to_datetime(df['time'])
```

### 61. 字符串类型转化为时间类型

```python
str1 = '2014-11-18'
datetime.strptime(str1, '%Y-%m-%d')
```

### 62. 回收

```python
import gc
del df # 逻辑删除
gc.collect() # 实际删除
```

### 63. pandas原始样本按天采样，更改为按其他标准重采样，并画出图线

```python
# 实战：沪市指数预测--arma

df_month = df.resample('M').mean()
print(df_month.head())
df_Q = df.resample('Q-DEC').mean()#按季度
print(df_Q.head())
df_year = df.resample('A-DEC').mean()#按年
print(df_year.head())

fig = plt.figure(figsize=[15, 7])
plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签
plt.suptitle('沪市指数', fontsize=20)
plt.subplot(221)
plt.plot(df.Price, '-', label='按天')
plt.legend()
plt.subplot(222)
plt.plot(df_month.Price, '-', label='按月')
plt.legend()
plt.subplot(223)
plt.plot(df_Q.Price, '-', label='按季度')
plt.legend()
plt.subplot(224)
plt.plot(df_year.Price, '-', label='按年')
plt.legend()
plt.show()
```



### 64. pandas将原始数据的时间格式转换为能处理的格式

data.Timestamp = pd.to_datetime(data.Timestamp)

### 65. pandas将某列设置为索引

pd.set_index('某列')

### 66. 求两个序列的排列组合

```python
from itertools import product
ps = range(0,3)
qs = range(0,3)
parameters = product(ps,qs)
parameters_list = list(parameters)
print(parameters_list)
[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]
```

### 67. 将时间序列数据转换为适用于监督学习的数据

```python
"""
将时间序列数据转换为适用于监督学习的数据
给定输入、输出序列的长度
data: 观察序列
n_in: 观测数据input(X)的步长，范围[1, len(data)], 默认为1
n_out: 观测数据output(y)的步长， 范围为[0, len(data)-1], 默认为1
dropnan: 是否删除NaN行
返回值：适用于监督学习的 DataFrame
"""
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    n_vars = 1 if type(data) is list else data.shape[1]
    df = DataFrame(data)
    cols, names = list(), list()
    # input sequence (t-n, ... t-1)
    for i in range(n_in, 0, -1):
        cols.append(df.shift(i))
        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]
    # 预测序列 (t, t+1, ... t+n)
    for i in range(0, n_out):
        cols.append(df.shift(-i))
        if i == 0:
            names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]
        else:
            names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]
    # 拼接到一起
    agg = concat(cols, axis=1)
    agg.columns = names
    # 去掉NaN行
    if dropnan:
        agg.dropna(inplace=True)
    return agg

```

### 68. pangrank工具 图论工具-networkx  igraph

**igraph**：可以处理百万级节点的网络；NetworkX比igraph好用；igraph比NetworkX强大

import igraph

g = igraph.Graph.Read_GML('./football.gml')

igraph.plot(g)

print(g.community_label_propagation())

以下是NetworkX：



![image-20210420192425326](%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.assets/image-20210420192425326.png)

节点查询：G.nodes()获取图中所有节点，G.number_of_nodes()获取图中节点的个数。



* 图的创建

无向图，使用nx.Graph()来创建

有向图，使用nx.DiGraph()来创建

* 节点的增加、删除和查询

添加节点：使用G.add_node('A')，也可以使用G.add_nodes_from(['B','C','D','E'])

删除节点：使用G.remove_node(node)，也可以使用G.remove_nodes_from(['B','C','D','E'])

* 边的增加

G.add_edge("A", "B")添加指定的从A到B的边

G.add_edges_from 从边集合中添加

G.add_weighted_edges_from 从带有权重的边的集合中添加

参数为1个或多个三元组[u,v,w]作为参数，u、v、w分别代表起点、终点和权重

* 边的删除

G.remove_edge，G.remove_edges_from

* NetworkX的可视化布局：

  spring_layout：中心放射状

  circular_layout：在一个圆环上均匀分布节点

  random_layout：随机分布节点

  shell_layout：节点都在同心圆上

```python


import networkx as nx
# 创建有向图
G = nx.DiGraph() 
# 有向图之间边的关系
edges = [("A", "B"), ("A", "C"), ("A", "D"), ("B", "A"), ("B", "D"), ("C", "A"), ("D", "B"), ("D", "C")]
for edge in edges:
    G.add_edge(edge[0], edge[1])
    
# 有向图可视化
layout = nx.spring_layout(G)
nx.draw(G, pos=layout, with_labels=True, hold=False)
plt.show()
     
# alpha = 1 是经典模型；其余是随机浏览模型
# 计算简化模型的PR值
pr = nx.pagerank(G, alpha=1)
print("简化模型的PR值：", pr)
# 计算随机模型的PR值
pr = nx.pagerank(G, alpha=0.8)
print("随机模型的PR值：", pr)

```

​	以上代码对应的图是：

![image-20210515234354343](%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.assets/image-20210515234354343.png)

### 



### 69. textrank工具-关键词：jieba、对比ifidf-关键句：textrank4zh

textrank**最重要的是关键句**

* **提取关键词**-jieba

  jieba中使用TextRank提取关键词

  jieba.analyse.textrank(string, topK=20, withWeight=True, allowPOS=())

  string：待处理语句

  topK：输出topK个词，默认20

  withWeight：是否返回权重值，默认false

  allowPOS：是否仅返回指定类型，默认为空

* **对比tfidf**

  jieba中使用TF-IDF提取关键词

  jieba.analyse.extract_tags(sentence, topK=20, withWeight=True, allowPOS=())

  效果好于TextRank，考虑了IDF的情况，而TextRank倾向使用频繁词

  效率高于TextRank，TextRank基于图的计算和迭代较慢

* **提取关键句**-textrank4zh工具

  * 也可提取关键词，提取结果与jieba存在差异

  * TextRank生成摘要的原理：

    * 每个句子作为图中的节点

    * 如果两个句子相似，则节点之间存在一条无向有权边

    * 相似度=同时出现在**两个句子中的单词的个数/句子中单词个数求对数之和**

    （分母使用对数可以降低长句在相似度计算上的优势）

  ```python
  
  
  # 关键词
  from textrank4zh import TextRank4Keyword, TextRank4Sentence
  # 输出关键词，设置文本小写，窗口为2
  tr4w = TextRank4Keyword()
  tr4w.analyze(text=text, lower=True, window=2)
  print('关键词：')
  for item in tr4w.get_keywords(20, word_min_len=1):
      print(item.word, item.weight)
  
  # 输出重要的句子
  tr4s = TextRank4Sentence()
  tr4s.analyze(text=text, lower=True, source = 'all_filters')
  print('摘要：')
  # 重要性较高的三个句子
  for item in tr4s.get_key_sentences(num=3):
      # index是语句在文本中位置，weight表示权重
      print(item.index, item.weight, item.sentence)
  
  ```

  

### 70. 读取网页文字，提取关键词、关键句，生成词云-textrank4zh 

```python
# 11：news内my-url2textrankAndwordcloud

import requests
from bs4 import BeautifulSoup
from textrank4zh import TextRank4Keyword, TextRank4Sentence
import jieba
import jieba.analyse
import jieba.posseg as pseg#词性标注
from wordcloud import WordCloud

url = 'https://3w.huanqiu.com/a/c36dc8/3xqGPRBcUE6?agt=8'
html = requests.get(url,timeout=10)
content = html.content
# print(content)

soup = BeautifulSoup(content,'html.parser',from_encoding='utf-8')
text = soup.get_text()
print(text)

# 获取人物、地点
words = pseg.lcut(text)# 相比于cut，lcut是更准确的切分
# 人物集合、地点集合
news_person = {word for word,flag in words if flag == "nr"}
news_place = {word for word,flag in words if flag == 'ns'}
print('新闻中有的人物是：',news_person)
print('新闻中有的地点是：',news_place)

import re
text = re.sub('[^\u4e00-\u9fa5。，！：、]{3,}','',text)#不是汉字和。，！：、的，都用''替换
print(text)

# 去掉停用词
def remove_stop_words(f):
    stop_words = ['不是','现在','时候','没有']
    for stop_word in stop_words:
        f = f.replace(stop_word,'')
    return f

# 生成词云
import matplotlib.pyplot as plt
def create_word_cloud(f):
    f = remove_stop_words(f)
    seg_list = jieba.lcut(f)
    cut_text = ' '.join(seg_list)
    wc = WordCloud(max_words=100,width=2000,height=1200,font_path = './msyh.ttf')#font_path = './msyh_boot.ttf'
    wordcloud = wc.generate(cut_text)
    wordcloud.to_file('wordcloud.jpg')
    plt.imshow(wordcloud)
    plt.axis('off')
    plt.show()

# 生成云
create_word_cloud(text)

# 输出关键词，设置文本小写，窗口为2
tr4w = TextRank4Keyword()
tr4w.analyze(text=text, lower=True, window=3)
print('关键词：')
for item in tr4w.get_keywords(20, word_min_len=2):
    print(item.word, item.weight)


# 输出重要的句子
tr4s = TextRank4Sentence()
tr4s.analyze(text=text, lower=True, source = 'all_filters')
print('摘要：')
# 重要性较高的三个句子
for item in tr4s.get_key_sentences(num=3):
	# index是语句在文本中位置，weight表示权重
    print(item.index, item.weight, item.sentence)
```





### 71. Graph Embedding工具

Graph Embedding工具：

https://github.com/shenweichen/GraphEmbedding （同样是DeepCTR作者）

包括多种Graph Embedding算法，Deep Walk, Node2Vec等

* **DeepWalk**

from graphembedding.ge.models import DeepWalk

* **Node2Vec**

from graphembedding.ge.models import Node2Vec

```python


from graphembedding.ge.models import DeepWalk
import networkx as nx
import pandas as pd
import numpy as np
import random
from tqdm import tqdm
from sklearn.decomposition import PCA
from node2vec import Node2Vec
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('./seealsology-data.tsv', sep='\t')#tsv文件，分割符是'\t'
G = nx.from_pandas_edgelist(df, 'source', 'target',edge_attr=True,create_using=nx.Graph())
print(type(G))
print(len(G))# target有2399种

model = DeepWalk(G,walk_length=10, num_walks=5, workers=1)
result = model.train(window_size=4,iter=20)#'DeepWalk' object has no attribute 'fit'
embeddings = model.get_embeddings()
embeddings

print(result.wv.most_similar('critical illness insurance'))


def plot_nodes(word_list):
    # 每个节点的embedding为100维
    X = model.w2v_model.wv[word_list]#115 * 100维
    print(type(X))
    # 将100维向量减少到2维
    pca = PCA(n_components=2)
    result = pca.fit_transform(X)
    # print(result)
    # 绘制节点向量
    plt.figure(figsize=(12, 9))
    # 创建一个散点图的投影
    plt.scatter(result[:, 0], result[:, 1])
    for i, word in enumerate(word_list):
        # 用word注释点xy
        plt.annotate(word, xy=(result[i, 0], result[i, 1]))
    plt.show()

plot_nodes(model.w2v_model.wv.index_to_key)

len(model.w2v_model.wv.index_to_key)
len(model.w2v_model.wv['world values survey'])
```





其他工具：

https://github.com/eliorc/node2vec

`node2vec.Node2Vec(graph, dimensions=64, walk_length=30, num_walks=200, workers=4) `

dimensions, embedding维数，默认为128

walk_length，节点随机游走的步长，默认为80

num_walks，每个节点随机游走次数，默认为10

p，返回概率参数，默认为1

q，出入概率参数，默认为1

workers，并行线程，默认为1

quiet，是否打印计算过程，默认为False，即输出计算过程

`Node2Vec.fit(window=4, iter=20)`

window，窗口大小，表示当前词与预测词在一个句子中的最大距离

iter，迭代次数



* 见GCN使用

### 72. pandas的可视化

* 缺失值可视化

  ```python
  # 项目：二手车交易价格预测
  
  import missingno as msno
  sample = train_data.sample(1000)
  msno.matrix(sample)
  plt.show()
  msno.bar(sample)
  plt.show()
  msno.heatmap(sample)
  plt.show()
  
  ```

  

* 一行生成报告

  ```python
  # 一行代码生成报告
  import pandas_profiling as pp
  report = pp.ProfileReport(train_data)
  # 导出为html
  report.to_file('report.html')
  ```

  

### 73. GCN使用-也是一种Graph Embedding

关键词：单位矩阵；由度生成对角矩阵

```python


# 不掉GCN包，从0写起
# 使用简单numpy实现GCN
import networkx as nx
from networkx import to_numpy_matrix
import matplotlib.pyplot as plt
import numpy as np

# 对网络G进行可视化
def plot_graph(G):
    plt.figure()
    pos = nx.spring_layout(G)
    edges = G.edges()
    nx.draw_networkx(G, pos, edgelist=edges)
    nx.draw_networkx_nodes(G, pos, nodelist=G.nodes(), node_size=300, node_color='r', alpha=0.8)
    nx.draw_networkx_edges(G, pos, edgelist=edges,alpha =0.4)
    plt.show()

# 数据加载，构造图
G = nx.read_gml('football.gml')
#print(G)
#print(type(G))

# 可视化
plot_graph(G)
print(list(G.nodes()))
print(G.nodes['BrighamYoung']['value'])
#print(G.nodes)
#print(G.nodes[:,'value'])


# 构建GCN，计算A_hat和D_hat矩阵
order = sorted(list(G.nodes()))
#按照字母顺序排序
print(order)

A = to_numpy_matrix(G, nodelist=order)
#邻接矩阵
print('A=\n', A)
# 生成对角矩阵，单位矩阵
I = np.eye(G.number_of_nodes())
A_hat = A + I
print('A_hat=\n', A_hat)

# D_hat为A_hat的度矩阵
D_hat = np.array(np.sum(A_hat, axis=0))[0]#[0]之前是(1,115)，取[0]是115维
print('D_hat=\n', D_hat)

# 得到对角线上的元素
D_hat = np.matrix(np.diag(D_hat))
print('D_hat=\n', D_hat)

# 初始化权重, normal 正态分布 loc均值 scale标准差
W_1 = np.random.normal(loc=0, scale=1, size=(G.number_of_nodes(), 4))
W_2 = np.random.normal(loc=0, size=(W_1.shape[1], 2))
print('W_1=\n', W_1)
print('W_2=\n', W_2)

# x<0时 结果=0; x>=0时，结果=x
def relu(x):
    return (abs(x)+x)/2

# 叠加GCN层，这里只使用单位矩阵作为特征表征，即每个节点被表示为一个 one-hot 编码的类别变量
def gcn_layer(A_hat, D_hat, X, W):
    # **-1求逆矩阵
    # 仅对角线上有非零值的矩阵  的逆矩阵  是 该矩阵每个数值的倒数
    return relu(D_hat**-1 * A_hat * X * W)
H_1 = gcn_layer(A_hat, D_hat, I, W_1)
H_2 = gcn_layer(A_hat, D_hat, H_1, W_2)
output = H_2
print('output=\n', output)

# 提取特征表征
feature_representations = {}
nodes = list(G.nodes())
for i in range(len(nodes)):
    feature_representations[nodes[i]] = np.array(output)[i]
print('feature_representations=\n', feature_representations)

# 不同节点value，绘制不同的颜色
def getValue(value):
    colorList = ['blue','green','purple','yellow','red','pink','orange','black','white','gray','brown','wheat']
    return colorList[int(value)]

# 绘制output，节点GCN embedding可视化
def plot_node(output, title):
    for i in range(len(nodes)):
        node_name = nodes[i]
        value = G.nodes[node_name]['value']
        plt.scatter(np.array(output)[i,0],np.array(output)[i,1] ,label=str(i),color = getValue(value),alpha=0.5,s = 250)
        plt.text(np.array(output)[i,0],np.array(output)[i,1] ,i, horizontalalignment='center',verticalalignment='center', fontdict={'color':'black'})
    plt.title(title)
    plt.show()
plot_node(output, 'Graph Embedding')

# 尝试去掉激活函数relu，重新运行一遍，发现效果反而更好
def gcn_layer(A_hat, D_hat, X, W):
    return D_hat**-1 * A_hat * X * W
H_1 = gcn_layer(A_hat, D_hat, I, W_1)
H_2 = gcn_layer(A_hat, D_hat, H_1, W_2)
output = H_2
print("output(去掉relu)=\n:",output)

plot_node(output, 'Graph Embedding without relu')
```



```python
# 以pytorch为例，大概说明
from skorch.classifier import NeuralNetBinaryClassifier

# 使用1层GCN层
H_list_model_1 = []
class Model_1_network(nn.Module):    
    def __init__(self, input_dim_1, input_dim_2, weight_dim_2, dropout_rate):        
        super(Model_1_network, self).__init__()
        self.input_dim_1 = input_dim_1
        self.input_dim_2 = input_dim_2
        self.weight_dim_1 = input_dim_2
        self.weight_dim_2 = weight_dim_2
        self.dropout_rate = dropout_rate
        # 定义GCN层，AX其实理解成X中每个节点对周围的加权，然后再过线性变换
        self.gcn = GCN_network(self.weight_dim_1, self.weight_dim_2)
        # 定义dropout
        self.dropout = nn.Dropout(p = self.dropout_rate)
        # 定义FC层,从隐层到1维
        self.fc = nn.Linear(self.input_dim_1 * self.weight_dim_2, 1)
    # 前向传播
    def forward(self, X):
        # 通过X得到A_hat, D_hat
        A_hat, D_hat = norn_adj(X, input_dim_1)
        X = to_one_hot(X, input_dim_1)
        # 将A_hat, D_hat, X作为输入，传入GCN层
        H = self.gcn(A_hat, D_hat, X)
        H = self.dropout(H)
        H = torch.relu(H)
        # 变成两维,第一个维度保持不变,后面的很多维全部变成一维
        H = H.view(H.size(0), -1)    
        # Visualization
        if not self.training:
            H_list_model_1.append(H.cpu())        
        H = self.fc(H)                
        return H.squeeze()

# 使用GCN做二分类
model_1 = NeuralNetBinaryClassifier(
    Model_1_network,
    module__input_dim_1 = input_dim_1,
    module__input_dim_2 = input_dim_2,    
    module__weight_dim_2 = weight_dim_2,
    module__dropout_rate = dropout_rate,
    batch_size = batch_size,
    max_epochs = max_epochs,
    train_split = None,
    optimizer = torch.optim.Adam,
    iterator_train__shuffle = True,
    device = 'cuda'
)
```



### 74. 0-1分类问题中正负样本不均衡

如果正样本数：负样本数 = 40:1，怎么训练?

loss函数是交叉信息熵公式，将公式中负样本那部分加上乘以权重40。

### 75. 模型评估

* roc曲线

  ```python
  # 12：api_call_sequence_detection_gcn  恶意软件检测
  
  from sklearn.metrics import precision_score, f1_score, roc_auc_score, roc_curve
  
  fpr_1, tpr_1, thresholds_1 = roc_curve(y_test, X_test_predictions_1)
  pyplot.plot([0, 1], [0, 1], linestyle='--')
  pyplot.plot(fpr_1, tpr_1)
  pyplot.show()
  ```

  

### 76. KNN工具

见13：启发式算法

from sklearn.neighbors import KNeighborsClassifier

from sklearn.neighbors import KNeighborsRegressor

KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30)

n_neighbors：即KNN中的K值，代表的是邻居的数量，默认值5

K值过小，会造成过拟合，K值过大，欠拟合

weights：是用来确定邻居的权重，有三种方式：

weights='uniform'，代表所有邻居的权重相同；

weights='distance'，代表权重是距离的倒数，即与距离成反比；

algorithm：用来规定计算邻居的方法，它有四种方式：

algorithm='auto'，根据数据的情况自动选择适合的算法，默认情况选择auto；

algorithm='kd_tree'，也叫作KD树，是多维空间的数据结构，方便对关键数据进行检索（KD树适用于维度少的情况，一般维数不超过20，如果维数大于20之后，效率反而会下降）

algorithm='ball_tree'，也叫作球树，它和KD树一样都是多维空间的数据结果，不同于KD树，球树更适用于维度大的情况；

algorithm='brute'，也叫作暴力搜索，它和KD树不同的地方是在于采用的是线性扫描，而不是通过构造树结构进行快速检索。当训练集大的时候，效率很低

leaf_size：代表构造KD树或球树时的叶子数，默认是30，调整leaf_size会影响到树的构造和搜索速度

### 77. eval() 函数

13：启发式算法-ppt Project A：基于能力描述的薪资预测

* 返回传入字符串的表达式的结果

* 变量赋值时，等号右边的表示是写成字符串的格式，返回值就是这个表达式的结果

### 78. random.sample使用

13：启发式算法-ppt Project A：基于能力描述的薪资预测

用于截取列表的指定长度的随机数

```python
list = [0,1,2,3,4]
rs = random.sample(list, 2)
print(rs) # [1,2]
print(list) # [0,1,2,3,4]
```



### 79. matplotlib画图使用

Matplotlib中plt.rcParams用法（设置图像细节）

* 设置中文字体为黑体

plt.rcParams['font.sans-serif'] = ['SimHei']

* 用来正常显示负号

plt.rcParams['axes.unicode_minus'] = False

### 80. python操作mysql的工具：sqlalchemy使用

见14：路径规划

```python
from sqlalchemy.orm import sessionmaker
# 创建&返回session
def get_db_session():
    engine = create_engine('mysql+mysqlconnector://root:password@localhost:3306/movielens')
    # 创建DBSession类型:
    DBSession = sessionmaker(bind=engine)
    # 创建session对象:
    session = DBSession()
    return engine, session

# 查询数据
sql_stmt = 'SELECT id, title, movie_id FROM movie'
rows = session.execute(sql_stmt)
# 写入或更新数据：
session.execute(insert_stmt, {'title': title, 'movie_href': movie_href, 'pic_href': pic_href})
session.commit()
```

### 81. python多线程工具：threading使用

Python提供了多个模块支持多线程编程，包括thread，threading和Queue模块等（推荐使用threading）

* start()

start方法开启一个新线程。把需要并行处理的代码放在run()方法中，start()方法启动线程将自动调用 run()方法

* join()

join所完成的工作就是线程同步，即主线程任务结束之后，进入阻塞状态，一直等待其他的子线程执行结束之后，主线程在终止

```python
# 创建一个threading类
class myThread (threading.Thread):
    def __init__(self, threadID):
```

### 82. pandas对某列数据差分

```python
df['新增一列'] = df['某列'].diff(1)# 一阶差分，参数'1'：有一个时间间隔

```

### 83. 神经网络中的embedding直接使用tf中的embedding

### 84. 创造、增加新特征

* 聚合->透视->将首行名转换为新的一列

  项目：天猫复购预测

  ```python
  groups = user_log.groupby(['merchant_id'])
  groups['action_type'].value_counts()
  '''
  merchant_id  action_type
  1            0              14009
               2                862
  ...             
  '''
  groups['action_type'].value_counts().unstack()
  '''
  action_type		0	1	2	3
  merchant_id				
  		  1		14009.0	14.0	862.0	644.0
  		  2		58.0	NaN	2.0	3.0
  '''
  groups['action_type'].value_counts().unstack().reset_index()
  '''
  action_type	merchant_id		0		1		2		3
  		0			1		14009.0	14.0	862.0	644.0
  		1			2		58.0	NaN		2.0		3.0
  '''
  后4列可作为新的特征
  ```

  

### 85. 训练集测试集划分-两种k折交叉验证 kfold StratifiedKFold

* StratifiedKFold 分层采样交叉切分，确保训练集，测试集中各类别样本的比例与原始数据集中相同。比如原数据中，0,1两类比例是1:1，通过观察StratifiedKFold切分的每个测试集可以发现，0,1两类的占比也为1:1，这就是分层采样。确定了测试集后，测试集的补集就是训练集。

* 同时应注意到每个测试集均是互斥的。

https://www.jianshu.com/p/1fee8c96fb29


```python
# 比赛：天猫复购预测
from sklearn.model_selection import StratifiedKFold
# 构造训练集和测试集
def get_train_testDF(train_df,label_df):
    skv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    trainX = []
    trainY = []
    testX = []
    testY = []
    for train_index, test_index in skv.split(X=train_df, y=label_df):
        train_x, train_y, test_x, test_y = train_df.iloc[train_index, :], label_df.iloc[train_index], \
                                            train_df.iloc[test_index, :], label_df.iloc[test_index]

        trainX.append(train_x)
        trainY.append(train_y)
        testX.append(test_x)
        testY.append(test_y)
    return trainX, testX, trainY, testY
```

### 86. 模型融合

在项目：二手车交易价格预测  中 united 1

### 87. 图论工具-networkx

* 生成图的方式：

  * 从tsv、csv文件生成图

    第十一章  seealsology-data

    ```python
    from graphembedding.ge.models import DeepWalk
    import networkx as nx
    import pandas as pd
    df = pd.read_csv('./seealsology-data.tsv', sep='\t')#tsv文件，分割符是'\t'
    G = nx.from_pandas_edgelist(df, 'source', 'target',edge_attr=True,create_using=nx.Graph())
    print(type(G))
    print(len(G))# target有2399种
    '''
    文件字段
    source	        target	                        depth
    life insurance	corporate-owned life insurance	1
    life insurance	critical illness insurance	1
    '''
    
    model = DeepWalk(G,walk_length=10, num_walks=5, workers=1)
    result = model.train(window_size=4,iter=20)#'DeepWalk' object has no attribute 'fit'
    embeddings = model.get_embeddings()
    ```

    

### 88. Graph Embedding工具

Graph Embedding工具：

https://github.com/shenweichen/GraphEmbedding （同样是DeepCTR作者）

包括多种Graph Embedding算法，Deep Walk, Node2Vec等

DeepWalk

from graphembedding.ge.models import DeepWalk

Node2Vec

from graphembedding.ge.models import Node2Vec

### 89. pandas 聚合函数groupby的多项统计功能联合使用 agg

```python
#项目：二手车交易价格预测
# as_index=False   sql式：重新生成索引
train_data.groupby('brand',as_index=False)['price'].agg(
        {'brand'+'_count':'count','brand'+'_price_max':'max'})
'''
	brand	brand_count	brand_price_max
0	0		31480		11.134604
1	1		13794		11.511935
'''

# as_index=True    dataframe式：label(本段代码中是brand)作为索引
train_data.groupby('brand',as_index=True)['price'].agg(
        [('brand'+'_count','count'),('brand'+'_price_max','max')])
'''

		brand_count		brand_price_max
brand		
0		31480			11.134604
1		13794			11.511935
'''
```

### 90. 热力图绘制  heatmap

```python
# 项目：二手车交易价格预测
# annot=True 显示相关系数的值
corr1 = abs(df[df['price'].notnull()][num_fea].corr())
plt.figure(figsize=(10, 10))
sns.heatmap(corr1, annot=True,linewidths=0.1, cmap=sns.cm.rocket_r)
```

### 91. svm SVC 后如果想要用predict_proba接口，必须sklearn.svm.SVC( probability=True )

### 92. 保存模型，加载模型

```python
# 模型的保存与加载
import joblib
rfc = RandomForestClassifier(n_estimators=100,max_depth=100)
rfc.fit(X,y)
#save model
joblib.dump(rfc, 'saved_model/rfc.pkl')
# 文件夹saved_model要提前创建好

#load model
model_lgb = joblib.load('saved_model/model_lgb.pkl')
```

### 93. 小样本采样

9：cnews中文文本分类

```python
# 将数据集变成小样本
filename = 'cnews.train.txt'
num_cat = {}
num_max = 100

# 读取文件
contents, labels = [], []
with open(filename, 'r', encoding='utf-8', errors='ignore') as f:
    for line in f:
        label, content = line.strip().split('\t')
        #print(label)
        if content:
            if label not in num_cat:
                num_cat[label] = 1
                contents.append(content)
                labels.append(label)
            else:
                if num_cat[label] < 100:
                    num_cat[label] = num_cat[label] + 1
                    contents.append(content)
                    labels.append(label)

# 写文件
with open('cnews.train.small.txt', 'w', encoding='utf-8', errors='ignore') as f:
    for content, label in zip(contents, labels):
        f.write(label + '\t' + content+'\n')
    f.close()
print(len(contents))
print(contents[0])
print(contents[1])
print(num_cat)
```

### 94. 归一化与反归一化 normalize minmaxscaler standardscaler

```python
# 实战：股票预测
sc = MinMaxScaler(feature_range=(0,1))
training_set_scaled = sc.fit_transform(training_set)
test_set = sc.transform(test_set)# 利用训练集的属性对测试集进行归一化

# 测试集输入模型进行预测
predicted_stock_price = model.predict(x_test)
# 对预测数据还原---从（0，1）反归一化到原始范围
predicted_stock_price = sc.inverse_transform(predicted_stock_price)
```

### 95. tensorflow断点续训

```python
# 实战：股票预测 或者 tf2.1学习笔记

```

### 96. airbnb 论文-12
