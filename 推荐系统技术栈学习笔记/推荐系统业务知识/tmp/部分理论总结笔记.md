### 1. FM的核心思想

![image-20210417204127620](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210417204127620.png)是FM的核心思想，使得稀疏数据下学习不充分的问题也能得到充分解决 => 可提供的非零样本大大增加



### 3. DNN一般3层，设置为200-400个比较适合

### 4. DeepFM与NFM

deepfm：fm + dnn    两者并行

nfm：lr + (fm->dnn)  两者并行，后半部分：fm和dnn串行

### 5. fm和svm的区别

* SVM的二元特征交叉参数是独立的，而FM的二元特征交叉参数是两个k维的向量vi、vj，交叉参数就不是独立的，而是相互影响的

* 在数据稀疏的情况下，线性SVM在和多项式SVM效果比较差，线性svm只有一维特征，不能挖掘深层次的组合特征；而多项式svm，交叉的多个特征需要在训练集上共现才能被学习到，否则该对应的参数就为0

* 而FM不一样，通过向量化的交叉，可以学习到不同特征之间的交互，进行提取到更深层次的抽象意义

### 补：adaboost和GBDT的区别？

精度和稳定性

都是boosting

GBDT拟合残差；

adaboost：加大上一轮训练出错的样本权重；加大训练准确的树的权重 

### 6. GBDT与XGBoost

XGBoost目标函数：

![image-20210517152556806](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210517152556806.png)

![image-20210517152637672](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210517152637672.png)

![image-20210518093620869](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210518093620869.png)

目标函数的极小值：

![image-20210518094009281](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210518094009281.png)

分枝增益：Gain = 分裂前 - 分裂后，可见Gain越大越好；由于有负号，所以相减得下式：

![image-20210518094101159](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210518094101159.png)

举例：

![image-20210518094203101](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210518094203101.png)

相比于GBDT：

* 目标函数在损失函数的基础上，增加**正则化项**：叶子数量 与 【（叶子分数的平方）之和】

* 损失函数使用**泰勒展开**，到**二阶**偏导

* 求出目标函数**obj的极值**，作为结构分数->可能还是通过SGD得到

* GBDT的分枝条件：使用树本身的分枝条件，例如纯度（可用方差表示）；

  XGBoost的分枝条件：分枝前后**结构分数的增益Gain越大**，越好；如果Gain<0，此叶节点不做分割

XGBoost特点：特征列排序后存储在内存，并行计算

XGBoost优化：

* 对于连续型特征值，样本数量非常大，该特征取值过多时，遍历所有取值会花费很多时间，且容易过拟合->分桶，桶边界上的特征值作为分枝节点候选

### 7. XGBoost与LightGBM

2017年经微软推出，XGBoost的升级版

Kaggle竞赛使用最多的模型之一，必备机器学习神器

Light => 在大规模数据集上运行效率更高

GBM => Gradient Boosting Machine

两者对比：

* 模型精度：两个模型相当
* 训练速度：LightGBM训练速度更快 => 1/10
* 内存消耗：LightGBM占用内存更小 => 1/6
* 特征缺失值：两个模型都可以自动处理特征缺失值
* 分类特征：**XGBoost不支持类别特征，需要对其进行OneHot编码，而LightGBM支持分类特征**

**XGBoost模型的复杂度：**

* 模型复杂度 = 树的棵数 X 每棵树的叶子数量 X 每片叶子生成复杂度

* 每片叶子生成复杂度 = 特征数量 X 候选分裂点数量 X 样本的数量

**针对XGBoost的优化：**

* Histogram算法，直方图算法 => 减少候选分裂点数量

  按照**连续特征的数值**分桶，每个桶内所有的gi和hi相加(i对应xi)，得到该桶的Gi和Hi(i对应bini)，

* GOSS算法，基于梯度的单边采样算法 => 减少样本的数量

  * 目标函数增益主要来自于梯度绝对值较大的样本

  * 单边采样，只对**梯度绝对值较小的样本按照一定比例采样，而保留了梯度绝对值较大的样本**

  * gi：梯度绝对值，假设阈值为0.1

    对gi<0.1的样本 => 1/3概率进行采样

    对gi>=0.1的样本 => 全部保留

    ![image-20210418151728270](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210418151728270.png)

* EFB算法，互斥特征捆绑算法 => 减少特征的数量

  * 思想是特征中包含大量稀疏特征的时候，减少构建直方图的特征数量，从而降低计算复杂度
  * 数据集中通常会有**大量的稀疏特征（大部分为0，少量为非0）**我们认为这些稀疏特征是互斥的（eg. feature1和feature2没有关联），即不会同时取非零值

  ![image-20210418152629816](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210418152629816.png)

* LightGBM = **XGBoost + Histogram + GOSS + EFB**

### 8. cnn几个基本概念

* **局部感受野**

* **共享权重**：每个感受野的kernel是同一个

* **特征图**：每种kernel去卷积图像，就反应了图像的不同特征；100种卷积核就有100个Feature Map。这100个Feature Map就组成了一层神经元

* **池化**：减小运算量

* **cnn的网络层**：
  * **卷积层**，C，特征提取层，得到特征图，目的是使原信号特征增强，并且降低噪音；	
  * **池化层**，S，特征映射层，将C层多个像素变为一个像素，目的是在保留有用信息的同时，尽可能减少数据量
  * **光栅化**：为了与传统的多层感知器MLP全连接；最终的矩形图像拉直成一个列向量接入MLP

![image-20210418225535317](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210418225535317.png)

* **白化**-BN(Batch Normalization)

  * 神经网络对0附近的数据更敏感，但是随着网络层数的增加，特征数据会出现偏离0均值的情况。

    - **标准化**：可以使数据符合以0为均值，1为标准差的分布。把偏移的特征数据重新拉回到0附近。
    - **批标准化**：对一小批数据(batch) ，做标准化处理。使数据回归标准正态分布，常用在卷积操作和激活操作之间，激活操作后也可。

    可以通过下方的式子计算批标准化后的输出特征图，得到第k个卷积核的输出特征图(feature map)中的第i个像素点。批标准化操作，会让每个像素点进行减均值除以标准差的自更新计算。

* **丢弃**-dropout的作用

  随机丢弃部分参数，减小参数；降低噪音；防止过拟合

  为了缓解神经网络过拟合，在神经网络训练过程中，常把隐藏层的部分神经元按照一定比例从神经网络中临时舍弃，在使用神经网络时再把所有神经元恢复到神经网络中。

* 全零填充

  为保证卷积计算保持输入特征图的尺寸不变，可以使用全零填充，在输入特征图周围填充0

  卷积输出特征图维度计算公式

  SAME（全零填充）向上取整，例如2.3取3:
  $$
  padding =\frac{\text { 输入特征图边长 }}{\text { 步长 }}
  $$
  
  
    VALID（不全0填充）向上取整：

$$
  padding =\frac{\text {输入特征图边长 }-\text { 卷积核长 } +1}{\text {步长}}
$$
 	 TF描述是否全零填充：

​	    用参数`padding = 'SAME`'表示使用全零填充

​        用参数`padding = 'VALID'`表示不使用全零填充

* 神经网络防止过拟合的方法
  * 训练数据要充足
  * 合适的batch_size、epoch、学习率
  * dropout
  * 损失函数+正则化
  * 经典的机器学习模型中，分箱化是一种方法，可能在神经网络中也是一种方法

### 9. ResNet与DenseNet

残差block:

![image-20210518124909543](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210518124909543.png)

![image-20210518124928102](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210518124928102.png)

![image-20210518124951791](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210518124951791.png)

上图中的‘+’是对应元素做加法

**ResNet**：神经网络模型退化：发现56层卷积网络的错误率 > 20层卷积网络的错误率；5种常用深度18，34，50，101，152；**使用更多**

![image-20210518130920036](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210518130920036.png)

**DenseNet**：缓解了梯度消失；加强了特征传播；增强了特征复用；减少了参数量；4种深度121，169，201, 264

### 10. 图像深度学习中的数据增广

图像增广：翻转和裁剪；变化颜色；叠加多个图像增广方法

### 11. 时间序列模型

建立了观察结果与时间变化的关系，能帮预测未来一段时间内的结果变化情况

**机器学习模型**，包括AR、MA、ARMA、ARIMA，他们的结果图看起来是真实值的延迟

**神经网络模型**，用LSTM进行时间序列预测

* **AR**-Auto Regressive-自回归

  p阶；回归系数；ut-白噪声：一个期望为0，方差为常数的纯随机过程

  系数是自相关系数，如果小于0.5，就不用这种模型了 

  ![image-20210420095454198](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420095454198.png)

* **MA**-Moving Average-滑动平均模型

  ut-**白噪声**：一个期望为0，方差为常数的纯随机过程

  Xt 为第t天的股票价格，而Ut为第t天的新闻影响，当天的股票价格受当天的新闻影响，也受昨天的新闻影响（但影响力要弱些，所以要乘上系数）

  ![image-20210420095920165](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420095920165.png)

* **ARMA模型**

  p和q两个阶数，称为ARMA(p,q)模型

  AR解决**当前数据与后期数据之间的关系**；MA则可以解决**随机变动，即噪声问题**

  ![image-20210420100209301](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420100209301.png)

  **AIC准则**，也叫作赤池消息准则，是衡量统计模型拟合好坏的一个标准，**数值越小**代表模型拟合得越好

* **ARIMA模型**-Auto Regressive Integrated Moving Average-**差分**自回归滑动平均模型

  ARIMA(p,d,q)模型，其中d是差分阶数

  差分不平稳数据，差分平稳后再用ARMA模型，训练找到最优的(pdq)和模型参数，使用训练好的模型预测，并对差分还原

  *什么是差分？*

  y = x^2，若y=[1,4,9,16,25]，一阶差分得[3,5,7,9]，二阶差分得[2,2,2]，平稳了。

  **y = x^d，d阶差分会平稳**

### 12. 为什么需要RNN

概念：循环核

之前模型的不足：只能单独的处理一个个输入，**前一个输入和后一个输入完全没有关系**。对于一句话的理解，如果单独理解每个单词是不够的，需要处理这些单词连接起来的整个序列，**即前一个输入对后一个输入是有关系的**

RNN可以按照时间线展开

S(t)的值，不仅取决于x(t)，还取决于S(t-1)

![image-20210420155333324](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420155333324.png)

RNN在每个阶段都共享一个参数W

隐藏层激活函数通常采用tanh

### 13. LSTM

![image-20210420164012579](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420164012579.png)

![image-20210420164351721](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420164351721.png)

![image-20210420164601331](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420164601331.png)

### 14. LSTM与传统RNN对比

* **传统循环网络RNN**可以通过记忆体**实现短期记忆**，进行连续数据的预测，但是当连续数据的序列变长时，会使展开**时间步过长**，在**反向传播更新参数**时，梯度要按照时间步连续相乘，会导致**梯度消失**

  ![image-20210420170058445](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420170058445.png)

  ![image-20210420170114432](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420170114432.png)

  所以：

  ![image-20210420170156605](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420170156605.png)

  ![image-20210420170231069](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420170231069.png)

  ![image-20210420170241786](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420170241786.png)

  根据上面一个式子可知：**RNN循环很深的时候，可能产生梯度消失和梯度爆炸**

* LSTM如何解决梯度消失？

  ![image-20210420172331882](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420172331882.png)

  ![image-20210420172418704](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420172418704.png)

  令![image-20210420172555287](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420172555287.png)

  **z的函数图像为：**

  ![image-20210420172619199](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420172619199.png)

  从图中看到，**Z的输出不是0就是1**，那么![image-20210420172803353](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420172803353.png)

  =>可以解决梯度消失问题

* LSTM通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息 => 适合长期记忆的任务

* LSTM隐藏层参数是RNN的4倍，使得**训练难度**加大了很多（Thinking：为什么是4倍）

  ![image-20210420171959041](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420171959041.png)

### 15. GRU

![image-20210421222441648](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210421222441648.png)



### 16. pagerank

核心10

* 简化版模型的步骤

![image-20210420190801364](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420190801364.png)



Step1，假设A、B、C、D的初始影响力相同，其实不相同也不影响最终的结果

Step2，进行第一次转移之后，页面的影响力变为

![image-20210420190834354](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420190834354.png)

Step3，进行n次迭代后，直到页面的影响力不再发生变化，也就是页面的影响力收敛=>最终的影响力

![image-20210420190939408](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420190939408.png)

* 简化版的两个问题

  * Rank Leak（等级泄露）->A吸收了所有能量

    ![image-20210420191147045](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420191147045.png)

  * Rank Sink（等级沉没）->A的能量最终为0

    ![image-20210420191250140](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420191250140.png)

* **随机浏览模型-引入阻尼因子d**：基于简化版模型的问题，提出解决办法

  d默认取值0.85

  ![image-20210420191447123](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420191447123.png)

  u：待评估的页面，*Bu为u的入链集合*。例如最上图：待估页面是A，Bu就是{B, C}
  
  对于Bu中的任意页面v，它能给u带来的影响力是其自身的影响力PR(v)除以**v页面的出链数量L(v)**。即页面v把影响力PR(v)平均分配给了它的出链。这样统计所有能给u带来链接的页面v，得到的总和即为u的影响力，即为PR(u)
  
  
  
  ```python
  def random_work(a, w, n):
  	d = 0.85
  	for i in range(100):
  		w = (1-d)/n + d*np.dot(a, w)
  		print(w)
  ```
  
  

### 17. textrank-基于pagerank

textrank是无向有权边

关键词与关键句

* 原理

  * Step1，分词和词性标注，去掉停用词，将单词添加到图中
  * Step2，出现在一个窗口中的词形成一条边
  * Step3，基于PageRank原理进行迭代（20-30次）
  * Step4，顶点（词）按照分数排序，可以筛选指定的词性

  ![image-20210420193437281](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420193437281.png)

  Wij: 单词i和j之间的权重

  节点的权重不仅依赖于入度，还依赖于入度节点的权重



textrank**最重要应用的是关键句**

* **对比tfidf**

  * 效果好于TextRank，考虑了IDF的情况，而TextRank倾向使用频繁词

  * 效率高于TextRank，TextRank基于图的计算和迭代较慢

* **提取关键句**-textrank4zh工具

  * 也可提取关键词，提取结果与jieba存在差异

  * TextRank生成摘要的原理：

    * 每个句子作为图中的节点

    * 如果两个句子相似，则节点之间存在一条无向有权边

    * 相似度=同时出现在**两个句子中的单词的个数/句子中单词个数求对数之和**

    （分母使用对数可以降低长句在相似度计算上的优势）

### 18. graph embedding

deep walk; node2vec; GCN图卷积神经网络

* **deep walk** = random walk + skip-gram

  random walk，即随机游走

  * 步骤：

    * 每个节点出发走10次，每次随机游走5步
    * 每个节点产生10个样本，每个样本是一个句子，该句子有5个"word"，总样本：N(节点个数) * 10
    * 每个样本即一个句子=>一个embedding

  * 推荐系统中的使用

    ![image-20210420212419397](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420212419397.png)

    用户行为中如何对物品进行Embedding：

    * 基于用户行为序列构建了物品相关图，物品A，B之间存在边是因为用户U1先后购买了物品A和物品B，所以产生了A => B的有向边。

    * **如果后续产生了多条相同的有向边，则有向边的权重被加强**。将所有用户行为序列都转换成物品相关图中的边之后，全局的物品相关图就建立起来了

    * 采用随机游走的方式随机选择起始点，重新产生物品序列

    * 最终将这些物品序列输入word2vec模型，生成最终的物品Embedding向量

* **node2vec**

  * **原理**：

  在DeepWalk基础上改进，控制BFS，DFS随机游走（带偏置的random walk策略）

  ![image-20210420213146771](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420213146771.png)

  从节点t跳转到节点v后，下一步从节点v跳转到周围各点的跳转概率![image-20210420213455472](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420213455472.png)

  w是边vx的权重，α是search bias，dtx是节点t到节点x 的距离：

  ![image-20210420213610337](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420213610337.png)

  ![image-20210420213705823](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210420213705823.png)

  **当p=1，q=1时，游走方式就等同于DeepWalk中的随机游走**

  * **特点：**
    * 调节p, q参数，可以调节BFS, DFS
    * 挖掘网络中的同质性（homophily） 和 结构性（structural equivalence）：同质性，指的是距离相近节点的embedding应该尽量近似，**节点u与相连的节点s1、s2、s3、s4的embedding表达应该是接近的** ；结构性，指的是结构上相似的节点的embedding应该相似，**节点u和节点s6都是各自局域网络的中心节点，embedding表达相近** 
    * 参数 $p$ 被称为**返回参数（Return Parameter）**，$p$ 越小，随机游走回节点 $t$ 的可能性越大，Node2vec 就更注重表达网络的结构性。参数 $q$ 被称为**进出参数（In-out Parameter）**，$q$ 越小，随机游走到远方节点的可能性越大，Node2vec 更注重表达网络的同质性。反之，当前节点更可能在附近节点游走。
  * 算法步骤：
    * Step1，转移概率（每个顶点到其周围顶点去的概率，由上面的pq超参数计算）矩阵可以提前算好
    * Step2，对图中的每个节点，进行r次随机游走得到r个长为l的walk，可以理解成每个给每个节点造句，r个l长的句子，全部节点的游走合起来就得到了这个网络的语料库
    * Step3，经过类似skip-gram的训练可得节点的Embedding

* **GCN**

  拉普拉斯矩阵是对称矩阵，可以特征分解

  ![image-20210421115624425](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210421115624425.png)

  GCN是一个神经网络层，层与层之间的传播方式：

  ![image-20210421115926410](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210421115926410.png)

  ![image-20210421120002761](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210421120002761.png)

  * 对上式的几个说明：

    * 为何矩阵A要加上单位矩阵I？

      A的对角线上都是0，所以在和特征矩阵H相乘的时候，只会计算一个node的所有邻居的特征的加权和，该node本身的特征却被忽略了。=>加上I

    * 波浪A前后的波浪D有什么作用？

      A是没有经过归一化的矩阵，如果A与特征矩阵H相乘会改变特征原本的分布，产生一些不可预测的问题。式子中的处理会得到一个**对称且归一化**的矩阵

  * RS中的使用

    不同类型的节点（比如用户、item）都可以放到一张图中，训练得到embedding

### 19. learning to rank : LTR

* 推荐不仅是预估问题，更应该关注排序问题

* LTR学习的三种策略：

  * Pointwise，针对单一文档

    LR, LR+GBDT,  SVM

  * Pairwise，关注文档的顺序关系

  * Listwise，将一次Query对应的所有搜索结果列表作为一个训练样例

### 20. 淘宝定向广告演化

名企13

DNNbase模型-->DIN模型-->DIEN模型-->DSIN模型

* base模型

  ![](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210421211503106.png)

  特征表示：

  * feature field是单值特征 => one-hot编码

  * feature field是多值特征 => multi-hot编码

  ![image-20210421210948761](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210421210948761.png)

* DIN模型

  * 模型结构

    MLP中的激活函数使用了PRelu/Dice->softmax

    ![image-20210421205502699](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210421205502699.png)

  ​		**attention计算公式**：

  ​	![image-20210421210402240](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210421210402240.png)

  *问题：为什么注意力不用softmax函数做归一化呢？*

  不对点击序列的Attention分数做归一化，直接将分数与对应商品的embedding向量做加权和，目的在于保留用户的兴趣强度。用SoftMax不能体现用户的行为强度，比如90%时间看衣服，10%看电子产品。

  * 评价指标-改进的AUC

    不是将所有用户的正负样本在一起计算，而是对于每个用户单独计算自身的AUC，并根据其自身的行为数量（如点击）进行加权处理

    ![image-20210421213611922](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210421213611922.png)

    

* DIEN

  * 结构

    从上到下依次是：

    * 兴趣转移层
    * 兴趣抽取层
    * 行为序列层：得到id的embedding
    
    ![image-20210422125302596](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210422125302596.png)
    
    attention计算：
    
    ![image-20210422130108972](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210422130108972.png)
    
    辅助损失函数：当前时刻的兴趣直接影响了下一时刻行为的发生（下一个行为就是当前行为的预估的正例）；loss采用负对数似然，负例的选择从用户未交互过的商品中随机抽取，或者从已展示给用户但没有点击的商品中随机抽取；使用auxiliary loss 进行额外的监督，用下一时刻的行为监督当前时刻兴趣的学习，促使GRU在提炼兴趣表达上更高效。

* DSIN

  对比DIEN改进点：**session会话**；兴趣抽取 由GRU->**self-attention**，该层的最终的输出一部分经由激活单元拼接到最终的embedding中，另一部分输入到其上的兴趣演化层；兴趣演化层捕捉不同session之间的关系 由AUGRU->双向的**LSTM**（bi-LSTM），该层的最终的输出经由激活单元拼接到最终的embedding中。

  * bi是embedding向量
  * 利用用户的行为序列中的多个历史会话，一个**session**是在给定的时间范围内发生的交互列表（用户行为列表）；和airbnb一样，即将用户的点击行为按照时间排序，前后的时间间隔大于30min，算成另一个session
  * 对每个Session用multi-head self-attention（多头自注意力）机制捕获**行为之间内部关系**，减少不相关行为的影响
  * bias encoding是行为之间的位置信息
  * Add代表了Residual Connection，为了解决多层神经网络训练困难的问题，通过将前一层的信息无差的传递到下一层，可以有效的只关注差异部分（ResNet中常用到）
  * Norm代表了Layer Normalization，对Layer的激活值的归一化 => 加速模型的训练过程，更快收敛
  * 可能 每个session内的输出做avg得到 I。https://zhuanlan.zhihu.com/p/136919871
  * 用户的session之间仍保持着一种序列关系，因此希望利用Bi-LSTM模型来捕捉session之间的关系；下图中第k个session Qk==>第k个I（即Ik）
  * 激活单元：
    * ![image-20210519170930607](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210519170930607.png)
    * ![image-20210519170904749](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210519170904749.png)

​	![image-20210422152506668](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210422152506668.png)



### 21. self-attention : 用在DISN、和seq2seq中

关键词：q是query；k是key；只有W们是需要学习得到的；multihead self-attention；positional  vector各向量之间的位置信息->DISN模型中添加biase encoding；self-attention在graph

![image-20210422140229739](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210422140229739.png)

**如何得到b1？**

![image-20210422140331453](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210422140331453.png)

**如何得到 attention α撇？**

![image-20210422141119323](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210422141119323.png)

* 矩阵运算

  ![image-20210422141810370](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210422141810370.png)

  ![image-20210422142027774](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210422142027774.png)

  ![image-20210422142342089](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210422142342089.png)

* multihead self-attention : 以二头为例，分了两个叉

  可能相关性不止一种，所以分多头
  
  ![image-20210422143922256](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210422143922256.png)



* self-attention in graph

  有共边的两个节点，才按照self-attention计算方法计算attention，没有共边的两个节点之间的attention是0.

### 22. self-attention 与 RNN比较

* sa中每个 输入向量对应的输出向量  都会考虑  该输入向量前后  所有的输入向量；经典RNN之后考虑输入向量前面的向量，不会考虑后面的向量==>现在的RNN有双向的，所以不是重要区别
* RNN中假如要最右侧的输出要要考虑最左侧的输入，该输入就必须沿着memory往后一层层传递；sa不需要
* 并行化：RNN 输出一排向量必须要依次进行；sa一排输出向量可并行计算出来

![image-20210422151159843](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210422151159843.png)

### 23. 评分卡模型

关键词：按照IV值筛选特征；筛选出来的是已分箱的特征，若某个样本在该特征维度上属于第i个箱子，那么该样本在该特征维度上输入LR的值是WOEi。

* woe编码-Weight of Evidence-证据权重

  event  bad

  ![image-20210423145658316](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210423145658316.png)

  B代表风险客户；G代表正常客户 

  将上式做两种变换：

  ![image-20210423192438335](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210423192438335.png)

  第一种变换的理解：**每个分箱里的坏人分布相对于好人分布之间的差异性**。

  ![image-20210423192557372](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210423192557372.png)

  第二种变换的理解：**每个分箱里的坏好比(Odds)相对于总体的坏好比之间的差异性**。

  

  **防止分母为0**，改进公式得到：

  ![image-20210423191405016](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210423191405016.png)

  * 实例计算

  *对于二分类问题共100条记录，一个自变量只有两个值value1, value2，如何计算value1, value2对应的woe1, woe2？*

  | 记录value | Label=1  个数 | Label=0  个数 | Label=1  的比率 | Label=0  的比率 | Woe                        |
  | --------- | ------------- | ------------- | --------------- | --------------- | -------------------------- |
  | Value1    | 40            | 10            | 40/(40+25)=62%  | 10/(10+25)=28%  | Ln(62%/28%)=ln(2.2)=0.79   |
  | Value2    | 25            | 25            | 25/(40+25)=38%  | 25/(10+25)=72%  | Ln(38%/72%)=ln(0.52)=-0.64 |

  **WOE差异越大，对风险区分能力越明显**

  * WOE在业务中常有的应用
    * **处理缺失值**：当数据源没有100%覆盖时，那就会存在缺失值，此时可以把null单独作为一个分箱。这点在分数据源建模时非常有用，可以有效将覆盖率哪怕只有20%的数据源利用起来。
    * **处理异常值**：当数据中存在离群点时，可以把其通过分箱离散化处理，从而提高变量的鲁棒性（抗干扰能力）。例如，age若出现200这种异常值，可分入“age > 60”这个分箱里，排除影响。
    * **业务解释性**：我们习惯于线性判断变量的作用，当x越来越大，y就越来越大。但实际x与y之间经常存在着**非线性**关系，此时可经过WOE变换。

* IV - Information Value

  我的理解：某个特征X可以分成X0，X1,...Xi...Xn-1共n箱，第i箱的IVi指该箱内bad比例比good比例多的程度。

  ![image-20210423194456885](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/image-20210423194456885.png)

  评估变量的预测能力，在应用实践中，其评价标准如下：

  ![preview](%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0.assets/v2-b32464c31bd592f53431ffe9cbc5e804_r.jpg)

* 实例：计算每个bucket中的WOE和IV

  margin_bad_rate = bad/total_bads

  margin_good_rate = good/total_goods

  WOE=ln(margin_bad_rate/margin_good_rate)

  IV=(bad/total_bads - good/total_goods)*WOE

  | **bucket** | **min_score** | **max_score** | **obs** | **bad** | **good** | **bad_rate** | **good_rate** | **margin_bad_rate** | **margin_good_rate** | **odds(bad/good)** | **woe**      | **IV**      |
  | ---------- | ------------- | ------------- | ------- | ------- | -------- | ------------ | ------------- | ------------------- | -------------------- | ------------------ | ------------ | ----------- |
  | 1          | 0             | 18            | 1390    | 70      | 1320     | 5.04%        | 94.96%        | 39.77%              | 15.09%               | 0.053030303        | 0.969204613  | 0.239234241 |
  | 2          | 18            | 23            | 1070    | 33      | 1037     | 3.08%        | 96.92%        | 18.75%              | 11.85%               | 0.031822565        | 0.45851674   | 0.031618681 |
  | 3          | 23            | 28            | 1162    | 20      | 1142     | 1.72%        | 98.28%        | 11.36%              | 13.05%               | 0.017513135        | -0.13870773  | 0.002345237 |
  | 4          | 28            | 34            | 1162    | 15      | 1147     | 1.29%        | 98.71%        | 8.52%               | 13.11%               | 0.013077594        | -0.430758529 | 0.019766824 |
  | 5          | 34            | 44            | 1212    | 12      | 1200     | 0.99%        | 99.01%        | 6.82%               | 13.72%               | 0.01               | -0.699073799 | 0.048230774 |
  | 6          | 44            | 100           | 1153    | 9       | 1144     | 0.78%        | 99.22%        | 5.11%               | 13.08%               | 0.007867133        | -0.938965208 | 0.074775794 |
  | 7          | null          | null          | 1775    | 17      | 1758     | 0.96%        | 99.04%        | 9.66%               | 20.10%               | 0.00967008         | -0.732622347 | 0.076463289 |
  | 总计       | 0             | 100           | 8924    | 176     | 8748     | 1.97%        | 98.03%        | 100.00%             | 100.00%              | 0.020118884        | 0            | 0.492434842 |

