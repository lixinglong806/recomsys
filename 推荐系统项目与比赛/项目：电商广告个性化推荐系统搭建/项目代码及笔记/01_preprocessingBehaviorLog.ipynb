{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理behavior_log数据集\n",
    "\n",
    "检查数据格式，以及对数据进行简单的统计运算(不同用户不同行为的次数)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 建立spark的session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 配置pyspark和spark driver运行时 使用的python解释器\n",
    "JAVA_HOME = '/root/bigdata/jdk'\n",
    "PYSPARK_PYTHON = '/miniconda2/envs/py365/bin/python'\n",
    "# 当存在多个版本时，不指定很可能会导致出错\n",
    "os.environ['PYSPARK_PYTHON'] = PYSPARK_PYTHON\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = PYSPARK_PYTHON\n",
    "os.environ['JAVA_HOME'] = JAVA_HOME\n",
    "# 配置spark信息\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_APP_NAME = 'preprocessingBehaviorLog'\n",
    "SPARK_URL = 'spark://192.168.58.100:7077'\n",
    "\n",
    "conf = SparkConf()    # 创建spark config对象\n",
    "config = (\n",
    "    (\"spark.app.name\", SPARK_APP_NAME),    # 设置启动的spark的app名称，没有提供，将随机产生一个名称\n",
    "    (\"spark.executor.memory\", \"2g\"),    # 设置该app启动时占用的内存用量，默认1g\n",
    "    (\"spark.master\", SPARK_URL),    # spark master的地址\n",
    "    (\"spark.executor.cores\", \"2\"),    # 设置spark executor使用的CPU核心数\n",
    "    # 以下三项配置，可以控制执行器数量\n",
    "    # (\"spark.dynamicAllocation.enabled\", True),\n",
    "    # (\"spark.dynamicAllocation.initialExecutors\", 1),    # 1个执行器\n",
    "    # (\"spark.shuffle.service.enabled\", True)\n",
    "    # ('spark.sql.pivotMaxValues', '99999'),  # 当需要pivot DF，且值很多时，需要修改，默认是10000\n",
    ")\n",
    "# 查看更详细配置及说明：https://spark.apache.org/docs/latest/configuration.html\n",
    "\n",
    "conf.setAll(config)\n",
    "\n",
    "# 利用config对象，创建spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 从hdfs上加载要处理的数据\n",
    "\n",
    "该数据集有20G，这里仅截取其中一部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 在hdfs上查看待处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "-rw-r--r--   1 root supergroup   31286431 2020-11-05 18:06 /data/ad_feature.csv\r\n",
      "drwxr-xr-x   - root supergroup          0 2020-11-06 15:38 /data/models\r\n",
      "-rw-r--r--   1 root supergroup 1088060964 2020-11-05 18:06 /data/raw_sample.csv\r\n",
      "-rw-r--r--   1 root supergroup   33812570 2020-12-12 18:39 /data/test1.csv\r\n",
      "-rw-r--r--   1 root supergroup   24056588 2020-11-05 18:06 /data/user_profile.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /data\n",
    "# test1.csv是behavior_log数据集截取的一部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 将待处理数据加载到spark，查看各列数据的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----+-----+------+\n",
      "|  user|time_stamp|btag| cate| brand|\n",
      "+------+----------+----+-----+------+\n",
      "|558157|1493741625|  pv| 6250| 91286|\n",
      "|558157|1493741626|  pv| 6250| 91286|\n",
      "|558157|1493741627|  pv| 6250| 91286|\n",
      "|728690|1493776998|  pv|11800| 62353|\n",
      "|332634|1493809895|  pv| 1101|365477|\n",
      "|857237|1493816945|  pv| 1043|110616|\n",
      "|619381|1493774638|  pv|  385|428950|\n",
      "|467042|1493772641|  pv| 8237|301299|\n",
      "|467042|1493772644|  pv| 8237|301299|\n",
      "|991528|1493780710|  pv| 7270|274795|\n",
      "|991528|1493780712|  pv| 7270|274795|\n",
      "|991528|1493780712|  pv| 7270|274795|\n",
      "|991528|1493780712|  pv| 7270|274795|\n",
      "|991528|1493780714|  pv| 7270|274795|\n",
      "|991528|1493780765|  pv| 7270|274795|\n",
      "|991528|1493780714|  pv| 7270|274795|\n",
      "|991528|1493780765|  pv| 7270|274795|\n",
      "|991528|1493780764|  pv| 7270|274795|\n",
      "|991528|1493780633|  pv| 7270|274795|\n",
      "|991528|1493780764|  pv| 7270|274795|\n",
      "+------+----------+----+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('/data/test1.csv',header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: string (nullable = true)\n",
      " |-- time_stamp: string (nullable = true)\n",
      " |-- btag: string (nullable = true)\n",
      " |-- cate: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 转换成可以处理的数据类型\n",
    "\n",
    "由上步知各列都是字符串型，将用于数字处理的类型转化成int和long类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, timestamp: bigint, btag: string, cateId: int, brandId: int]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "schema = StructType([\n",
    "    StructField('userId',IntegerType()),\n",
    "    StructField('timestamp',LongType()),\n",
    "    StructField('btag',StringType()),\n",
    "    StructField('cateId',IntegerType()),\n",
    "    StructField('brandId',IntegerType())\n",
    "])#给每列数据 重命名+变换数据类型，相当于Row()\n",
    "behavior_log_df=spark.read.csv('/data/test1.csv',header=True,schema=schema)\n",
    "behavior_log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----+------+-------+\n",
      "|userId| timestamp|btag|cateId|brandId|\n",
      "+------+----------+----+------+-------+\n",
      "|558157|1493741625|  pv|  6250|  91286|\n",
      "|558157|1493741626|  pv|  6250|  91286|\n",
      "|558157|1493741627|  pv|  6250|  91286|\n",
      "|728690|1493776998|  pv| 11800|  62353|\n",
      "|332634|1493809895|  pv|  1101| 365477|\n",
      "|857237|1493816945|  pv|  1043| 110616|\n",
      "|619381|1493774638|  pv|   385| 428950|\n",
      "|467042|1493772641|  pv|  8237| 301299|\n",
      "|467042|1493772644|  pv|  8237| 301299|\n",
      "|991528|1493780710|  pv|  7270| 274795|\n",
      "|991528|1493780712|  pv|  7270| 274795|\n",
      "|991528|1493780712|  pv|  7270| 274795|\n",
      "|991528|1493780712|  pv|  7270| 274795|\n",
      "|991528|1493780714|  pv|  7270| 274795|\n",
      "|991528|1493780765|  pv|  7270| 274795|\n",
      "|991528|1493780714|  pv|  7270| 274795|\n",
      "|991528|1493780765|  pv|  7270| 274795|\n",
      "|991528|1493780764|  pv|  7270| 274795|\n",
      "|991528|1493780633|  pv|  7270| 274795|\n",
      "|991528|1493780764|  pv|  7270| 274795|\n",
      "+------+----------+----+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "behavior_log_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- btag: string (nullable = true)\n",
      " |-- cateId: integer (nullable = true)\n",
      " |-- brandId: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 再次查看数据类型是否转成我们可以处理的类型\n",
    "behavior_log_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. spark处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 分析数据集字段的类型和格式\n",
    "a. 查看是否有空值\n",
    "<br>b. 查看每列数据的类型\n",
    "<br>c. 查看每列数据的类别情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| userId|count|\n",
      "+-------+-----+\n",
      "| 668074|    7|\n",
      "| 550961|    4|\n",
      "| 313148|    2|\n",
      "| 285987|    5|\n",
      "| 369968|    1|\n",
      "| 328078|    1|\n",
      "| 309510|    1|\n",
      "| 201031|    4|\n",
      "|1097471|   15|\n",
      "|1137432|    1|\n",
      "| 511225|    2|\n",
      "| 396208|   13|\n",
      "| 913040|    9|\n",
      "| 604834|    2|\n",
      "| 535839|    1|\n",
      "| 147711|   12|\n",
      "| 304448|    4|\n",
      "| 815397|    4|\n",
      "| 387467|    1|\n",
      "|1141195|    5|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "behavior_log_df.groupby('userId').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看userId的数据情况： 242109\n"
     ]
    }
   ],
   "source": [
    "# 查看总共用多少用户\n",
    "# 第一个count()，返回的是一个dataframe，这里的count计算的是每一个分组的个数，即每个用户有多少行行为\n",
    "# 完整的数据集有113w个用户\n",
    "print('查看userId的数据情况：',behavior_log_df.groupby('userId').count().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|btag| count|\n",
      "+----+------+\n",
      "| buy| 13668|\n",
      "| fav| 12807|\n",
      "|cart| 22142|\n",
      "|  pv|951383|\n",
      "+----+------+\n",
      "\n",
      "查看btag的数据情况： [Row(btag='buy', count=13668), Row(btag='fav', count=12807), Row(btag='cart', count=22142), Row(btag='pv', count=951383)]\n"
     ]
    }
   ],
   "source": [
    "behavior_log_df.groupby('btag').count().show()\n",
    "print('查看btag的数据情况：',behavior_log_df.groupby('btag').count().collect())#collect会将所有数据加载到内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看cateId的数据情况 6044\n"
     ]
    }
   ],
   "source": [
    "print('查看cateId的数据情况',behavior_log_df.groupby('cateId').count().count())\n",
    "# 商品类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看brandId的数据情况 49179\n"
     ]
    }
   ],
   "source": [
    "print('查看brandId的数据情况',behavior_log_df.groupby('brandId').count().count())\n",
    "# 商品品牌类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "判断数据是否有空值：\n",
      "原始数据有1000000行\n",
      "去掉空值后数据有1000000行\n"
     ]
    }
   ],
   "source": [
    "# pandas中选择某列使用df['某列的名字']，但是sparksql不能这样用，要使用sql语句，df.select('某列的名字')\n",
    "# 可以使用df['某列的名字'].cast(某种数据类型如Longtype())\n",
    "print('判断数据是否有空值：')\n",
    "print('原始数据有%d行'%behavior_log_df.count())\n",
    "#dropna()-某行数据有一个值是空值，就将该行删除，注意该方法不去掉str类型的null和NULL\n",
    "print('去掉空值后数据有%d行'%behavior_log_df.dropna().count())\n",
    "# 根据结构可知：没有空值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 统计每个用户对各类商品、各类品牌的pv、fav、cart、buy数量，建立透视表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pyspark.sql.GroupedData.pivot\n",
    "\n",
    "pivot透视操作，把某列里的字段值转换成行并进行聚合运算\n",
    "\n",
    "如果透视的字段中的不同属性值超过10000个，则需要设置spark.sql.pivotMaxValues，否则计算过程中会出现错误。[文档介绍](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=pivot#pyspark.sql.GroupedData.pivot)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计每个用户对各类商品的pv、fav、cart、buy数量\n",
    "# 建立透视表： | userId|cateId| pv| fav|cart| buy|\n",
    "cate_count_df=behavior_log_df.groupby(behavior_log_df.userId,behavior_log_df.cateId).pivot('btag',['cv','fav','cart','buy']).count()\n",
    "# cate_count_df.show()\n",
    "# tmp.where('userId=38456').show()\n",
    "# 条件选择where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- cateId: integer (nullable = true)\n",
      " |-- cv: long (nullable = true)\n",
      " |-- fav: long (nullable = true)\n",
      " |-- cart: long (nullable = true)\n",
      " |-- buy: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cate_count_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+----+----+----+\n",
      "| userId|brandId|  cv| fav|cart| buy|\n",
      "+-------+-------+----+----+----+----+\n",
      "| 454702|  98931|null|null|null|null|\n",
      "| 755246| 237649|null|null|null|null|\n",
      "|  59109| 293023|null|null|null|null|\n",
      "| 671713| 435218|null|null|null|null|\n",
      "| 915329| 383166|null|   1|null|null|\n",
      "| 161038| 247861|null|null|null|null|\n",
      "|  20612| 184921|null|null|null|null|\n",
      "|1078294| 113336|null|null|null|null|\n",
      "| 353660|  39211|null|null|null|null|\n",
      "| 726597| 102457|null|null|null|null|\n",
      "| 286584| 186296|null|null|null|null|\n",
      "|  60577|  93403|null|null|null|null|\n",
      "|1055041| 393034|null|null|null|null|\n",
      "|1056336| 352813|null|null|null|null|\n",
      "| 970730| 231690|null|null|null|null|\n",
      "| 743396| 310648|null|null|null|null|\n",
      "| 633672| 139148|null|null|null|null|\n",
      "| 921779| 168156|null|null|null|null|\n",
      "| 383183| 185225|null|null|null|null|\n",
      "| 187100| 238536|null|null|null|null|\n",
      "+-------+-------+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 统计每个用户对各个品牌的pv、fav、cart、buy数量\n",
    "# 建立透视表  | userId|brandId|  cv| fav|cart| buy|\n",
    "brand_count_df=behavior_log_df.groupby(behavior_log_df.userId,behavior_log_df.brandId).pivot('btag',['cv','fav','cart','buy']).count()\n",
    "brand_count_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 将透视表先存储起来\n",
    "  由于运算时间比较长，所以这里先将结果存储在hdfs，供后续其他操作使用\n",
    "  写入数据时才开始计算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_count_df.write.csv(\"cate_count.csv\", header=True)\n",
    "brand_count_df.write.csv(\"brand_count.csv\", header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
