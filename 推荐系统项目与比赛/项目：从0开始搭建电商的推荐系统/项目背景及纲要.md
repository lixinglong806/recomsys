## 1.项目背景

网站新上线时的主要推荐业务流实现

## 2. 业务架构

![image-20210320162921281](%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF%E5%8F%8A%E7%BA%B2%E8%A6%81.assets/推荐系统业务架构.png)

### 2.1 基础数据层

* 元数据收集：
  * 业务DB：网站业务DB内相关数据。以美多商城当前数据库中商品数据为基础
  * 日志数据：埋点日志。包括：点击流日志、曝光日志
* 推荐业务原始数据存储：HDFS
  * 业务DB ==> HDFS	工具：sqoop
  * 日志数据 ==> HDFS/KAFKA     工具：flume							
* 元数据简单的预处理

### 2.2 数据处理层

* 离线计算：基于内容的物品相似度计算	**物品画像、用户画像**	行为偏好评分	相关模型训练
  - **用户画像**：性别、年龄、品牌偏好、品类偏好、购买力等级、尺码颜色偏好、促销敏感度、资产状况、家庭情况等。
  - **商品画像**：商品的产品词、修饰词、属性词、品牌词、品类词、价格档次、适用人群性别、适用人群年龄等标签
* 在线计算：实时行为分析



### 2.3 推荐业务层

* 初期：没有用户行为数据，**冷启动**

  * 根据商品详细属性等数据，离线计算出商品画像（就是提取标签的过程）

  * **解决物品冷启动**：根据商品画像的部分属性，求出的**物品相似度**，推荐与排序

  * **解决用户冷启动**：

    * 根据用户**当日实时行为**数据为用户建立推荐结果集。例如，用户买了某类商品，就为该用户推荐 该类中其余商品若干。

    * **首页推荐**使用热门商品、新品等非个性化推荐方式

    - **详情页**以相似商品进行推荐
    - **购物车页面**以与购物车中相似的商品进行推荐
    - **搜索页面**根据搜索词关联进行推荐

* 中后期：利用积累的大量用户行为数据，用户画像更加精准，逐渐以**离线推荐为主，实时推荐为主**

  * 离线召回：
    * 根据用户点击流日志，预估用户对物品的评分，实现**协同过滤推荐**
    * 用户点击流日志+物品特征==> 提取用户行为特征，刻画**更加个性化的用户画像**
  * 离线模型：曝光日志信息+用户和物品画像==>点击率/转化率/跳出率等模型
  * 实时推荐：（在线+离线）召回集选出若干物品，带入CTR模型计算出点击流，依照点击流大小推荐。



## 3. 项目实现

### 3.1 业务数据处理

#### 3.1.1 业务db迁移

写sh脚本文件，使用sqoop进行迁移，分为以下两种：

* Transfer MySQL To HDFS
* Transfer MySQL To Hive
* spark可以直接访问MySQL业务数据库，获取数据，但是这样可能会导致业务数据库压力比较大，数据量较大时，不推荐这样处理。



**涉及的技术**：

* 如何写shell脚本并执行
* sqoop迁移的代码
* 为何开启hive前要开启mysql？hive元数据记录着hive中存储着哪些数据库，哪些表等信息，通常是使用mysql数据库来存储hive的元数据 ，因此要开启hive前要开启mysql。
* spark中从Hive数据库加载数据，如何配置？
* spark直接访问MySQL业务数据库

#### 3.1.2 商品SKU数据整合

将商品品的**所有详细描述组合成一个文本**，以便接下来提取关键词

* 电商SKU的表

![电商SKU的几张表](%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF%E5%8F%8A%E7%BA%B2%E8%A6%81.assets/%E7%94%B5%E5%95%86SKU%E7%9A%84%E5%87%A0%E5%BC%A0%E8%A1%A8.png)

* 以tb_sku表为基础，将其他表中的信息合并  

  合并后保留字段：sku_id | name | caption | category1_id | category2_id | category3_id | price | cost_price | market_price | specification | category1 | category2 | category3  

  - sku_id | name | caption | price | cost_price | market_price | goods_id 来自**tb_sku表**
  - category1_id | category2_id | category3_id 来自**tb_goods表** 
  - category1 | category2 | category3 根据tb_goods表中的类别ID 找到**tb_goods_category表**中的类别名 获得
  - specification字段 将以**tb_sku_specifition表** 为基础，合并**tb_goods_specification表**和**tb_specification_option表**某些字段获得  

  **注**：商品推荐 将以SKU为最小单位，SKU-stock keeping unit即“库存量单位”-最小可用单位。SKU是指一款商品，每款都有出现一个SKU，便于电商品牌识别商品； **很多时候，一款商品多色，则是有多个SKU**

* 整合完后的dataframe -> 临时注册表（df不能使用sql语句，因此将df转化成临时注册表） ->spark.sql使用hql语句创建hive表并插入数据 -> 存到hdfs



**涉及的技术**：

* `spark.sql()`中的语句是hive使用的hql语句，不是mysql使用的sql语句。
* 将多个字段的信息去重、拼接成一列信息，涉及的hql语句：group by、concat、concat_ws、collect_set、sort_array

#### 3.1.3 提取商品关键词：textrank法与tfidf法

* 使用上面整合好的表，从表中**商品详细信息**列提取关键词并算出关键词权重
* 同一个关键词在不同品类数据之间通常都具有不同的含义，往往需要根据品类特性，分别解析、提取关键词。
* 使用两种提取方法tfidf、textrank，将它们的权重结果求平均，作为某关键词的最终权重

* 最终的表：|sku_id|industry(品类)| tag(关键词)| textrank|tfidf|weights(权重)|。存入hive



**涉及的技术**：

* 电商的商品归类
* python2推荐用codecs.open，Python3用open，故推荐用codecs.open
* nlp中的分词+过滤
* 结巴分词
* textrank
* spark中的计算tfidf

#### 3.1.4 关键词的词向量(word2vec)

**涉及的技术**：

* spark如何使用w2v



#### 3.1.4 基于内容的商品相似度计算与商品召回

* 计算商品的向量：

  1件sku 有n个关键词，1个关键词(带有1个权重)有 1个embedding词向量表示，**该sku的商品表示 = [  (第i词的词向量表示\*第i词的权重)加和    ] / 该sku有多少个关键词**  

* 利用**皮尔逊相似度**得出 每件商品 与之相似的 topN件商品，存入redis

**涉及的技术：**

* sparl.mlib计算皮尔逊相似度
* redis
* 【可忽略，延申学习用】pyspark中有多种相似度计算API，都只能完成所有列之间的相似度计算，行行之间需要自行实现



### 3.2 日志数据处理

#### 3.2.1 埋点日志概述

**埋点**：在应用中的特定流程，收集一些信息，用来跟踪应用使用的状况，后续用来进一步优化产品或是提供运营的数据支撑。

埋点方式主流的无非两种方式：

- 自行研发：在研发的产品中注入代码进行统计，并搭建起相应的后台查询和处理
- 第三方平台：第三方统计工具，如友盟、百度移动等

**埋点日志**主要分为类型：

- 曝光日志：商品被展示到页面被称为曝光，曝光日志也就是指商品一旦被展示出来，则记录一条曝光日志
    - 曝光时间
    - 曝光场景
    - 用户唯一标识
    - 商品ID
    - 商品类别ID
- 点击流日志：用户浏览、收藏、加购物车、购买、评论、搜索等行为记录日志
    - 被曝光时间：对应曝光日志的曝光时间(浏览)
    - 被曝光场景：对应曝光日志的曝光场景(浏览)
    - 用户唯一标识
    - 行为时间
    - 行为类型
    - 商品ID
    - 商品类别ID
    - 停留时长(浏览)
    - 评分(评论)
    - 搜索词(搜索)

**埋点日志意义**：

* 用户行为偏好分析
  * 利用点击流日志分析个体/群体用户的行为特征，预测出用户行为的偏好

* 统计指标分析

  * 点击率：顾名思义被点击的概率，计算公式通常是：点击次数/曝光次数。如某商品共曝光或展示了100次，曝光后总共被点击了10次，那么点击率则是10%

  * 跳出率：用户访问一个页面后，之后没有再也没有其他操作，称为跳出，计算公式常用的是：访问一次就退出的访问量/总的访问量
    * 整体(整个板块/应用)跳出率
    * 单页面的跳出率。如某页面共计有100个用户访问，但其中有10个用户访问当前页面后就再也没有其他访问了，那么当前页面的跳出率是10%

  * 转化率：电商中的转化率计算：商品订单成交量/商品访问量。如某商品累计访问量是100个，最终提交订单的只有5个，那么该商品转化率就是5%

**注意**：跳出率和转化率中的访问量指的是**独立用户访问量**。独立用户访问量：首先独立用户访问量并不等价于来访问的总用户个数。比如某用户A在1月1日访问了商品1，1月2日又访问了商品1，那么这里商品1的访问量应算作2次，而不是1次。

#### 3.2.2 实时点击流日志

* source：在本地建立一个`click_trace.log文件`
* sink：hdfs、kafka
* 基于`logging.getlogger()类`，自定义一个get_logger函数，用于往`click_trace.log文件`写自己设定的内容，作为新增的日志。
* `click_trace.log文件` 新增内容 => flume（采集`click_trace.log文件`里新增的内容，发送到） =>  kafka（使用StreamingContext**对象ssc**和设置的**kafka参数**，生成dstream）  =>   ssc.start之后使用   transform、action算子 处理rdd（dstream是一批一批的rdd）

**涉及的技术**：

* 使用`logging.getlogger()类`，往某个文件里写内容
* flume配置文件
* kafka

#### 3.2.3 离线埋点日志

* flume配置的sink是kafka和hdfs，因此**曝光日志**和**点击日志**都会存到hdfs中
* spark读取hdfs存放的**曝光日志**和**点击日志**，分别提取有用信息。
* 根据提取的信息，分析得到更多信息。
  * 例如：曝光日志和点击流日志对齐后，利用点击流日志中行为是"pv"的数据，同时cate_id|exposure_loc|exposure_timesteamp|sku_id|uid一一对应的数据，最终就能得出：所有**曝光的商品中，哪些商品被用户浏览了，哪些没有被浏览**



### 3.3 画像建模处理

画像建模，简言之，就是给用户/商品打标签，那么具体打些什么样的标签：
- 用户画像主要展现用户的兴趣标签（长期兴趣、短期兴趣、实时兴趣）。兴趣主要包括性别、年龄、品牌偏好、品类偏好、购买力等级、尺码颜色偏好、促销敏感度、资产状况、家庭情况等。
- 商品画像主要包括商品的产品词、修饰词、属性词、品牌词、品类词、价格档次、适用人群性别、适用人群年龄等标签

**画像中大部分数据都设置为数值**，这里主要原因有两个：1. 节约存储空间 2. 便于进行特征处理



#### 3.3.1 用户画像

**画像类型**：

- 长期兴趣画像：根据用户所有的历史行为记录来刻画
- 短期兴趣画像：根据用户近期的历史行为记录来刻画
- 实时兴趣画像：根据用户实时的历史行为记录来刻画

长期兴趣和短期兴趣主要用来进行离线推荐，可以使用hive来进行维护实现离线推荐(离线召回、模型训练等任务)，而实时兴趣画像目的是为了进行实时推荐(在线召回、在线特征更新、在线排序)，因此会使用redis、hbase这样的数据库来存储

**标签**：

- 性别、年龄、教育状况等基于人口统计学的标签
- 品牌偏好、品类偏好、尺码颜色偏好等基于用户行为统计的标签
- 购买力等级、活跃度等基于规则的标签

同样的对于性别、年龄、教育状况、是否有孩子、孩子年龄、购买力等级、活跃度等可以设计到一张表中：
```sql
CREATE TABLE IF NOT EXISTS user_features_table(
user_id INT,
gender INT,
age_level INT,
education INT,
height DOUBLE,
has_child INT,
child_age_level INT,
comsume_level INT,
active_level INT
)
```

对于用户品牌偏好、品类偏好、尺码偏好、颜色偏好等，如果数据量中等可以全部存在一张表中，如果用户量大，那么应该拆分表来存储。如
```sql
CREATE TABLE IF NOT EXISTS user_brand_table(
user_id INT,
brand_id INT,
weights DOUBLE
)

CREATE TABLE IF NOT EXISTS user_cate_table(
user_id INT,
cate_id INT,
weights DOUBLE
)
......
```
这样做的话可以提高查询速度，解耦存储逻辑等。当然也会带来的一些问题，如表连接、聚合操作较多等等。



**用户画像的六大维度特征：**

![用户画像的六大维度特征](%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF%E5%8F%8A%E7%BA%B2%E8%A6%81.assets/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E7%9A%84%E5%85%AD%E5%A4%A7%E7%BB%B4%E5%BA%A6%E7%89%B9%E5%BE%81.png)



#### 3.3.2 商品画像

**定义商品画像标签**：

- 产品词、修饰词、属性词等基于商品提取的关键词
- 品牌词、品类词、价格档次等基于商品属性
- 适用人群性别、年龄等基于商品购买历史、商品特性

建立商品画像表: 商品画像不同的标签往往根据特性会分为多个表进行存储

商品的品牌词(品牌1、2、3)、品类词(类别1、2、3)、价格档次(高:1、中:2、低:3)、适用人群性别(男:1、女:2)、年龄(少年:1、青年:2、中年:3等)等标签，每个商品通常只对应某一个具体的值，因此这样的标签可以设计在一张表中存储。如
```sql
CREATE TABLE IF NOT EXISTS item_features_table(
sku_id INT,
brand_id INT,
cate_id INT,
price_level INT, 
gender INT, 
age_level INT
)
```

而商品的产品词、修饰词、属性词，由于每个商品都会有多个词与之对应，且取值范围无法固定，因此每个商品的这些词会在多行中进行存储。而其实此前我们对每件商品提取出的关键词，就包括刚刚提到的产品词、修饰词、属性词。如

```sql
CREATE TABLE IF NOT EXISTS item_keywords_table(
sku_id INT,
cate STRING,
brand STRING,
keyword STRING,
weights DOUBLE
)
```

#### 3.3.4  部分标签值计算说明

**物品的价格等级**：item_features_table中`price_level`的计算：

- 首先划分等级，如分为两级、三级还是五级
- 然后最简单的方式就是根据直接使用价格排序方式，直接划分出三个或五个等不同的等级



**用户的消费等级**：comsume_level

- 同样首先划分等级，通常和商品价格等级相同
- 然后根据用户购买历史来进行加权，得出用户总的消费等级情况，当然如果想要刻画更精细的画像，可以考虑把用户每一个大类的消费等级计算出来进行保存



**用户活跃度**：active_level

- 同样首先划分等级，如不活跃、活跃、非常活跃
- 指定规则，比如：当发现用户近七天天登录，并发生多次点击浏览行为，可记录为非常活跃；当发现用户最近7天只有三天有类似行为，则为活跃；否则为不活跃。从用户历史点击流日志数据中进行分析提取

用户对品类、品牌等的偏好权重的计算可参考此前项目1中的方式进行计算

其他还有众多的标签，尤其是用户的标签，往往都需要指定一套合适规则，然后从用户历史行为记录中查询分析出来，最终形成完成的用户画像。



### 3.4 推荐业务处理

* 初期：在**没有排序模型**情况下，可直接根据**商品的相似度**，找出 与用户当前正在发生行为的商品 相似的商品们，为该用户进行推荐(**在线召回**)

  * 前面（*3.1.4 基于内容的商品相似度计算与商品召回*）已经用redis为每个商品储存了top-N个相似商品
  * 根据用户**实时行为日志**数据为用户建立推荐结果集。例如，用户买了某个商品，就为该用户 推荐top-N个 与该商品相似的商品们。

* 中后期：利用积累的大量用户行为数据，逐渐以**离线推荐为主，实时推荐为主**

  * 离线处理，训练出相关排序模型(点击率预测、跳出率预测、转化率预测)。

  * 由于**离线推荐**主要以T+1形式推荐，因此**在线推荐**就还需要对用户今日的行为进行统计分析，得出用户今日的实时兴趣作为用户的实时画像，**供排序模型使用**

    

**备注**：T+1形式指今天之前是一个 T单位的数据,新加一天就是（T + 1）单位的数据。