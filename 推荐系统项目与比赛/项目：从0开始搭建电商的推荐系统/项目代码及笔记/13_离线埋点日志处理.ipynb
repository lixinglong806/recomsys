{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 离线埋点日志处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日志是有固定格式的，使用正则匹配将日志转换为我们处理过的spark sql dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面已经把点击流日志存储hdfs中，这里再实现对曝光日志的采集，但注意曝光日志只需要发送到hdfs即可\n",
    "\n",
    "`/root/bigdata/flume/conf/exposure_log_hdfs.properties`:\n",
    "```\n",
    "# Name the components on this agent\n",
    "a1.sources = r1\n",
    "a1.sinks = k1\n",
    "a1.channels = c1\n",
    "\n",
    "# Describe/configure the source\n",
    "a1.sources.r1.type = exec\n",
    "a1.sources.r1.command = tail -F /root/meiduoSourceCode/logs/exposure.log\n",
    "a1.sources.r1.channels = c1\n",
    "\n",
    "a1.sources.r1.interceptors = t1\n",
    "a1.sources.r1.interceptors.t1.type = timestamp\n",
    "\n",
    "# Use a channel which buffers events in memory\n",
    "a1.channels.c1.type = memory\n",
    "a1.channels.c1.capacity = 1000\n",
    "a1.channels.c1.transactionCapacity = 100\n",
    "\n",
    "a1.sinks.k1.type = hdfs\n",
    "a1.sinks.k1.channel = c1\n",
    "a1.sinks.k1.hdfs.path = hdfs://localhost:9000/project2-meiduo-rs/logs/exposure/%y-%m-%d\n",
    "as.sinks.k1.hdfs.userLocalTimeStamp = true\n",
    "a1.sinks.k1.hdfs.filePrefix = exposure-\n",
    "a1.sinks.k1.hdfs.fileType = DataStream\n",
    "a1.sinks.k1.hdfs.writeFormat = Text\n",
    "a1.sinks.k1.hdfs.round = true\n",
    "a1.sinks.k1.hdfs.roundValue = 10\n",
    "a1.sinks.k1.hdfs.roundUnit = minute\n",
    "```\n",
    "\n",
    "启动flume：`flume-ng agent -f /root/bigdata/flume/conf/exposure_log_hdfs.properties -n a1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 配置pyspark和spark driver运行时 使用的python解释器\n",
    "JAVA_HOME = '/root/bigdata/jdk'\n",
    "PYSPARK_PYTHON = '/miniconda2/envs/py365/bin/python'\n",
    "# 当存在多个版本时，不指定很可能会导致出错\n",
    "os.environ['PYSPARK_PYTHON'] = PYSPARK_PYTHON\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = PYSPARK_PYTHON\n",
    "os.environ['JAVA_HOME'] = JAVA_HOME\n",
    "# 配置spark信息\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_APP_NAME = \"processingSKUMetadata\"\n",
    "SPARK_URL = \"spark://192.168.58.100:7077\"\n",
    "\n",
    "conf = SparkConf()    # 创建spark config对象\n",
    "config = (\n",
    "\t(\"spark.app.name\", SPARK_APP_NAME),    # 设置启动的spark的app名称，没有提供，将随机产生一个名称\n",
    "\t(\"spark.executor.memory\", \"2g\"),    # 设置该app启动时占用的内存用量，默认1g，指一台虚拟机\n",
    "\t(\"spark.master\", SPARK_URL),    # spark master的地址\n",
    "    (\"spark.executor.cores\", \"2\"),    # 设置spark executor使用的CPU核心数，指一台虚拟机\n",
    "    (\"hive.metastore.uris\", \"thrift://localhost:9083\"),    # 配置hive元数据的访问，否则spark无法获取hive中已存储的数据\n",
    "    \n",
    "    # 以下三项配置，可以控制执行器数量\n",
    "#     (\"spark.dynamicAllocation.enabled\", True),\n",
    "#     (\"spark.dynamicAllocation.initialExecutors\", 1),    # 1个执行器\n",
    "#     (\"spark.shuffle.service.enabled\", True)\n",
    "# \t('spark.sql.pivotMaxValues', '99999'),  # 当需要pivot DF，且值很多时，需要修改，默认是10000\n",
    ")\n",
    "# 查看更详细配置及说明：https://spark.apache.org/docs/latest/configuration.html\n",
    "\n",
    "conf.setAll(config)\n",
    "\n",
    "# 利用config对象，创建spark session\n",
    "spark = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2020-12-23 23:04 /meiduo_mall/logs/click-trace/20-12-23\r\n",
      "drwxr-xr-x   - root supergroup          0 2020-12-24 11:36 /meiduo_mall/logs/click-trace/20-12-24\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /meiduo_mall/logs/click-trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_c0                                                                                                                                                    |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2018/12/01 02:41:57: exposure_timesteamp<1543603102> exposure_loc<detail> timesteamp<1543603317> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2018/12/01 02:43:18: exposure_timesteamp<1543603102> exposure_loc<detail> timesteamp<1543603398> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2018/12/01 02:43:19: exposure_timesteamp<1543603102> exposure_loc<detail> timesteamp<1543603399> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2018/12/01 02:43:20: exposure_timesteamp<1543603102> exposure_loc<detail> timesteamp<1543603400> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2018/12/01 02:43:21: exposure_timesteamp<1543603102> exposure_loc<detail> timesteamp<1543603401> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2018/12/01 13:13:28: exposure_timesteamp<1543641203> exposure_loc<detail> timesteamp<1543641208> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2018/12/01 13:16:01: exposure_timesteamp<1543641203> exposure_loc<detail> timesteamp<1543641361> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2020/12/24 11:35:03: exposure_timesteamp<1608780903> exposure_loc<detail> timesteamp<1608780903> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2020/12/24 11:35:03: exposure_timesteamp<1608780903> exposure_loc<detail> timesteamp<1608780903> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2020/12/24 11:35:03: exposure_timesteamp<1608780903> exposure_loc<detail> timesteamp<1608780903> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2020/12/24 11:35:03: exposure_timesteamp<1608780903> exposure_loc<detail> timesteamp<1608780903> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2018/12/01 13:16:03: exposure_timesteamp<1543641203> exposure_loc<detail> timesteamp<1543641363> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2020/12/23 21:32:33: exposure_timesteamp<1608729679> exposure_loc<detail> timesteamp<1608730353> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2020/12/24 11:09:22: exposure_timesteamp<1608779359> exposure_loc<detail> timesteamp<1608779362> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2020/12/24 11:09:22: exposure_timesteamp<1608779359> exposure_loc<detail> timesteamp<1608779362> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2020/12/24 11:33:42: exposure_timesteamp<1608780822> exposure_loc<detail> timesteamp<1608780822> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2020/12/24 11:33:58: exposure_timesteamp<1608780838> exposure_loc<detail> timesteamp<1608780838> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2018/12/01 02:41:56: exposure_timesteamp<1543603102> exposure_loc<detail> timesteamp<1543603316> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2018/12/01 02:41:56: exposure_timesteamp<1543603102> exposure_loc<detail> timesteamp<1543603316> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "|2018/12/01 02:41:56: exposure_timesteamp<1543603102> exposure_loc<detail> timesteamp<1543603316> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date = '20-12-24'\n",
    "click_trace = spark.read.csv('/meiduo_mall/logs/click-trace/%s'%date)\n",
    "click_trace.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|_c0                                                                                                  |\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|2018/11/30 03:19:41: exposure_timesteamp<1543519181> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2018/11/30 13:18:13: exposure_timesteamp<1543555093> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2018/11/30 13:18:13: exposure_timesteamp<1543555093> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2018/12/01 01:47:59: exposure_timesteamp<1543600079> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2018/12/01 01:49:20: exposure_timesteamp<1543600079> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2018/12/01 01:52:53: exposure_timesteamp<1543600373> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2018/12/01 01:57:47: exposure_timesteamp<1543600666> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2020/12/23 21:21:21: exposure_timesteamp<1608729679> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2020/12/24 11:09:20: exposure_timesteamp<1608779359> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2020/12/24 11:09:20: exposure_timesteamp<1608779359> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2018/11/30 03:19:41: exposure_timesteamp<1543519181> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2018/11/30 13:18:13: exposure_timesteamp<1543555093> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2018/11/30 13:18:13: exposure_timesteamp<1543555093> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2018/12/01 01:47:59: exposure_timesteamp<1543600079> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2018/12/01 01:49:20: exposure_timesteamp<1543600079> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2018/12/01 01:52:53: exposure_timesteamp<1543600373> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2018/12/01 01:57:47: exposure_timesteamp<1543600666> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2020/12/23 21:21:21: exposure_timesteamp<1608729679> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2020/12/24 11:09:20: exposure_timesteamp<1608779359> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "|2020/12/24 11:09:20: exposure_timesteamp<1608779359> exposure_loc<detail> uid<1> sku_id<1> cate_id<1>|\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date = '20-12-24'\n",
    "exposure = spark.read.csv('/meiduo_mall/logs/exposure/%s'%date)\n",
    "exposure.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 延伸学习：1.正则表达式匹配 2.日志如何转化spark sql dataframe\n",
    "# import re\n",
    "# s = '2018/12/01 02:35:13: exposure_timesteamp<1543601846> exposure_loc<detail> timesteamp<1543602913> behavior<pv> uid<1> sku_id<1> cate_id<1> stay_time<60>'\n",
    "\n",
    "# match = re.search(\"\\\n",
    "# exposure_timesteamp<(?P<exposure_timesteamp>.*?)> \\\n",
    "# exposure_loc<(?P<exposure_loc>.*?)> \\\n",
    "# timesteamp<(?P<timesteamp>.*?)> \\\n",
    "# behavior<(?P<behavior>.*?)> \\\n",
    "# uid<(?P<uid>.*?)> \\\n",
    "# sku_id<(?P<sku_id>.*?)> \\\n",
    "# cate_id<(?P<cate_id>.*?)> \\\n",
    "# stay_time<(?P<stay_time>.*?)>\", s)\n",
    "# match.groupdict()\n",
    "\n",
    "# {'exposure_timesteamp': '1543601846',\n",
    "#  'exposure_loc': 'detail',\n",
    "#  'timesteamp': '1543602913',\n",
    "#  'behavior': 'pv',\n",
    "#  'uid': '1',\n",
    "#  'sku_id': '1',\n",
    "#  'cate_id': '1',\n",
    "#  'stay_time': '60'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------------+-------------------+------+---------+----------+---+\n",
      "|behavior|cate_id|exposure_loc|exposure_timesteamp|sku_id|stay_time|timesteamp|uid|\n",
      "+--------+-------+------------+-------------------+------+---------+----------+---+\n",
      "|      pv|      1|      detail|         1543603102|     1|       60|1543603317|  1|\n",
      "|      pv|      1|      detail|         1543603102|     1|       60|1543603398|  1|\n",
      "|      pv|      1|      detail|         1543603102|     1|       60|1543603399|  1|\n",
      "|      pv|      1|      detail|         1543603102|     1|       60|1543603400|  1|\n",
      "|      pv|      1|      detail|         1543603102|     1|       60|1543603401|  1|\n",
      "|      pv|      1|      detail|         1543641203|     1|       60|1543641208|  1|\n",
      "|      pv|      1|      detail|         1543641203|     1|       60|1543641361|  1|\n",
      "|      pv|      1|      detail|         1608780903|     1|       60|1608780903|  1|\n",
      "|      pv|      1|      detail|         1608780903|     1|       60|1608780903|  1|\n",
      "|      pv|      1|      detail|         1608780903|     1|       60|1608780903|  1|\n",
      "|      pv|      1|      detail|         1608780903|     1|       60|1608780903|  1|\n",
      "|      pv|      1|      detail|         1543641203|     1|       60|1543641363|  1|\n",
      "|      pv|      1|      detail|         1608729679|     1|       60|1608730353|  1|\n",
      "|      pv|      1|      detail|         1608779359|     1|       60|1608779362|  1|\n",
      "|      pv|      1|      detail|         1608779359|     1|       60|1608779362|  1|\n",
      "|      pv|      1|      detail|         1608780822|     1|       60|1608780822|  1|\n",
      "|      pv|      1|      detail|         1608780838|     1|       60|1608780838|  1|\n",
      "|      pv|      1|      detail|         1543603102|     1|       60|1543603316|  1|\n",
      "|      pv|      1|      detail|         1543603102|     1|       60|1543603316|  1|\n",
      "|      pv|      1|      detail|         1543603102|     1|       60|1543603316|  1|\n",
      "+--------+-------+------------+-------------------+------+---------+----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 点击日志的有用信息转换成df\n",
    "import re\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def map(row):\n",
    "    match = re.search(\"\\\n",
    "exposure_timesteamp<(?P<exposure_timesteamp>.*?)> \\\n",
    "exposure_loc<(?P<exposure_loc>.*?)> \\\n",
    "timesteamp<(?P<timesteamp>.*?)> \\\n",
    "behavior<(?P<behavior>.*?)> \\\n",
    "uid<(?P<uid>.*?)> \\\n",
    "sku_id<(?P<sku_id>.*?)> \\\n",
    "cate_id<(?P<cate_id>.*?)> \\\n",
    "stay_time<(?P<stay_time>.*?)>\", row._c0)\n",
    "\n",
    "    row = Row(exposure_timesteamp=match.group(\"exposure_timesteamp\"),\n",
    "                exposure_loc=match.group(\"exposure_loc\"),\n",
    "                timesteamp=match.group(\"timesteamp\"),\n",
    "                behavior=match.group(\"behavior\"),\n",
    "                uid=match.group(\"uid\"),\n",
    "                sku_id=match.group(\"sku_id\"),\n",
    "                cate_id=match.group(\"cate_id\"),\n",
    "                stay_time=match.group(\"stay_time\"))\n",
    "    \n",
    "    return row\n",
    "click_trace.rdd.map(map).toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------------+------+---+\n",
      "|cate_id|exposure_loc|exposure_timesteamp|sku_id|uid|\n",
      "+-------+------------+-------------------+------+---+\n",
      "|      1|      detail|         1543519181|     1|  1|\n",
      "|      1|      detail|         1543555093|     1|  1|\n",
      "|      1|      detail|         1543555093|     1|  1|\n",
      "|      1|      detail|         1543600079|     1|  1|\n",
      "|      1|      detail|         1543600079|     1|  1|\n",
      "|      1|      detail|         1543600373|     1|  1|\n",
      "|      1|      detail|         1543600666|     1|  1|\n",
      "|      1|      detail|         1608729679|     1|  1|\n",
      "|      1|      detail|         1608779359|     1|  1|\n",
      "|      1|      detail|         1608779359|     1|  1|\n",
      "|      1|      detail|         1543519181|     1|  1|\n",
      "|      1|      detail|         1543555093|     1|  1|\n",
      "|      1|      detail|         1543555093|     1|  1|\n",
      "|      1|      detail|         1543600079|     1|  1|\n",
      "|      1|      detail|         1543600079|     1|  1|\n",
      "|      1|      detail|         1543600373|     1|  1|\n",
      "|      1|      detail|         1543600666|     1|  1|\n",
      "|      1|      detail|         1608729679|     1|  1|\n",
      "|      1|      detail|         1608779359|     1|  1|\n",
      "|      1|      detail|         1608779359|     1|  1|\n",
      "+-------+------------+-------------------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 曝光日志的有用信息转换成df\n",
    "import re\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def map(row):\n",
    "    match = re.search(\"\\\n",
    "exposure_timesteamp<(?P<exposure_timesteamp>.*?)> \\\n",
    "exposure_loc<(?P<exposure_loc>.*?)> \\\n",
    "uid<(?P<uid>.*?)> \\\n",
    "sku_id<(?P<sku_id>.*?)> \\\n",
    "cate_id<(?P<cate_id>.*?)>\", row._c0)\n",
    "    match.group\n",
    "    \n",
    "    row = Row(exposure_timesteamp=match.group(\"exposure_timesteamp\"),\n",
    "                exposure_loc=match.group(\"exposure_loc\"),\n",
    "                uid=match.group(\"uid\"),\n",
    "                sku_id=match.group(\"sku_id\"),\n",
    "                cate_id=match.group(\"cate_id\"))\n",
    "    \n",
    "    return row\n",
    "exposure.rdd.map(map).toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 曝光日志和点击流日志对齐\n",
    "\n",
    "利用点击流日志中行为是\"pv\"的数据同时`cate_id|exposure_loc|exposure_timesteamp|sku_id|uid`一一对应的数据，最终就能得出：所有曝光的商品中，哪些商品被用户浏览了，哪些没有被浏览"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
